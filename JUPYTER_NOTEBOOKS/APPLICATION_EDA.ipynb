{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "Directorio actual: c:\\Users\\Yeray\\Desktop\\DATA_SCIENCE_ML\\Home-Credit-TFG\\JUPYTER_NOTEBOOKS\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/bureau_balance_agg_def_v2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m app_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/application_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m bureau_final \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/bureau_agg_final.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m bureau_balance__agg_def_v1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/bureau_balance_agg_def_v2.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(app_train\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     32\u001b[0m       , bureau_final\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     33\u001b[0m       , bureau_balance__agg_def_v1\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/bureau_balance_agg_def_v2.csv'"
     ]
    }
   ],
   "source": [
    "#Celda para librerías\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(np.__version__)\n",
    "from ydata_profiling import ProfileReport\n",
    "import os\n",
    "print(\"Directorio actual:\", os.getcwd())\n",
    "\n",
    "# Have all columns appear when dataframes are displayed.\n",
    "pd.set_option('display.max_columns', None) \n",
    "# Have 100 rows appear when a dataframe is displayed\n",
    "pd.set_option('display.max_rows', 500)\n",
    "# Display dimensions whenever a dataframe is printed out.\n",
    "pd.set_option('display.show_dimensions', True)\n",
    "\n",
    "#Importando los datos\n",
    "\n",
    "app_train = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/application_train.csv')\n",
    "bureau_final = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/bureau_agg_final.csv')\n",
    "bureau_balance__agg_def_v1 = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/bureau_balance_agg_def_v2.csv')\n",
    "print(app_train.shape\n",
    "      , bureau_final.shape\n",
    "      , bureau_balance__agg_def_v1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uniendo los datos\n",
    "app_train = app_train.merge(bureau_final, on='SK_ID_CURR', how='left')\n",
    "app_train = app_train.merge(bureau_balance__agg_def_v1, on='SK_ID_CURR', how='left')\n",
    "\n",
    "app_train.drop(columns=['SK_ID_CURR'], inplace=True)\n",
    "#Debido a un fallo que detecta que todas los valores de la columna DAYS_BIRTH \n",
    "# son nan cuando no es así, vamos a sustituir la columna por una nueva llamada AGE_INT\n",
    "app_train['AGE_INT'] = app_train['DAYS_BIRTH']/-365\n",
    "app_train['HAS_CHILDREN'] = np.where(app_train['CNT_CHILDREN'] > 0, 1, 0)\n",
    "#variables_categoricas = app_train.select_dtypes(include=['object', 'category']).columns\n",
    "#app_train[variables_categoricas] = app_train[variables_categoricas].astype('category')\n",
    "#categoricas = variables_categoricas.tolist()\n",
    "info = app_train.info()\n",
    "print(info)\n",
    "#describe = app_train.describe()\n",
    "#print(describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "\n",
    "\n",
    "#Vamos a convertir la variable CNT_CHILDREN en una variable categórica que indique 1 si tiene hijos y 0 si no.\n",
    "\n",
    "#Vamos a crear la variable DAYS_EMPLOYED en una variable categórica que indique 1 si tiene empleo y 0 si no.\n",
    "app_train['HAS_EMPLOYMENT'] = app_train['DAYS_EMPLOYED'].map(lambda app_train: 1 if app_train < 0 else 0)\n",
    "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "\n",
    "\n",
    "#Voy a crear RATIOS para algunas de las variables más importantes (en mi opinión y según lo investigado) \n",
    "# y que además tienen valores atípicos.\n",
    "app_train['CREDIT_AMT_INCOME_RATIO'] = app_train['AMT_CREDIT']/app_train['AMT_INCOME_TOTAL']\n",
    "app_train['CREDIT_AMT_INCOME_PORCENTAJE'] = app_train['AMT_INCOME_TOTAL']/app_train['AMT_CREDIT']\n",
    "\n",
    "app_train['ANNUITY_AMT_INCOME_RATIO'] = app_train['AMT_INCOME_TOTAL']/app_train['AMT_ANNUITY']\n",
    "app_train['ANNUITY_INCOME_PORCENTAJE'] = app_train['AMT_ANNUITY']/app_train['AMT_INCOME_TOTAL']\n",
    "\n",
    "app_train['GOODS_INCOME_RATIO'] = app_train['AMT_GOODS_PRICE']/app_train['AMT_INCOME_TOTAL']\n",
    "\n",
    "app_train['CREDIT_ANNUITY_RATIO'] = app_train['AMT_CREDIT']/app_train['AMT_ANNUITY']\n",
    "app_train['ANNUITY_CREDIT_PORCETAJE'] = app_train['AMT_ANNUITY']/app_train['AMT_CREDIT']\n",
    "app_train['CREDIT_GOODS_PRICE_RATIO'] = app_train['AMT_CREDIT']/app_train['AMT_GOODS_PRICE']\n",
    "\n",
    "app_train['ANNUITY_TO_DAYS_EMPLOYED_RATIO'] = app_train['AMT_ANNUITY']/app_train['DAYS_EMPLOYED']\n",
    "app_train['ANNUITY_TO_CHILDREN_RATIO'] = app_train['AMT_ANNUITY']/app_train['CNT_CHILDREN']\n",
    "\n",
    "app_train['INCOME_PER_CHILD'] = app_train['AMT_INCOME_TOTAL']/app_train['CNT_CHILDREN']\n",
    "app_train['CREDIT_PER_CHILD'] = app_train['AMT_CREDIT']/app_train['CNT_CHILDREN']\n",
    "\n",
    "app_train['AMT_INCOME_FAMILY_MEMBERS_RATIO'] = app_train['AMT_INCOME_TOTAL']/app_train['CNT_FAM_MEMBERS']\n",
    "app_train['CHILDREN_RATIO'] = app_train['CNT_CHILDREN']/app_train['CNT_FAM_MEMBERS']\n",
    "app_train['ADULTS_MEMBERS_DIFF'] = app_train['CNT_FAM_MEMBERS'] - app_train['CNT_CHILDREN']\n",
    "\n",
    "app_train['INCOME_PER_ADULT'] = app_train['AMT_INCOME_TOTAL']/app_train['ADULTS_MEMBERS_DIFF']\n",
    "app_train['CREDIT_PER_MEMBER'] = app_train['AMT_CREDIT']/app_train['CNT_FAM_MEMBERS']\n",
    "app_train['CREDIT_PER_ADULT'] = app_train['AMT_CREDIT']/app_train['ADULTS_MEMBERS_DIFF']\n",
    "app_train['ANNUITY_PER_ADULT'] = app_train['AMT_ANNUITY']/app_train['ADULTS_MEMBERS_DIFF']\n",
    "\n",
    "\n",
    "#app_train['OWN_CAR_AGE_RATIO'] = app_train['OWN_CAR_AGE']/app_train['AGE_INT']\n",
    "app_train['NEW_CAR_BIRTH_RATIO'] = app_train['OWN_CAR_AGE']/app_train['DAYS_BIRTH']\n",
    "app_train['NEW_CAR_EMPLOYMENT_RATIO'] = app_train['OWN_CAR_AGE']/app_train['DAYS_EMPLOYED']\n",
    "\n",
    "\n",
    "app_train['EMPLOYMENT_AGE_RATIO'] = app_train['DAYS_EMPLOYED']/-app_train['AGE_INT']\n",
    "#app_train['EMPLOYMENT_BIRTH_RATIO'] = app_train['DAYS_EMPLOYED']/app_train['DAYS_BIRTH']\n",
    "app_train['EMPLOYMENT_PHONE_CHANGE_RATIO'] = app_train['DAYS_LAST_PHONE_CHANGE']/app_train['DAYS_EMPLOYED']\n",
    "\n",
    "app_train['NEW_PHONE_CHANGE_BIRTH_RATIO'] = app_train['DAYS_LAST_PHONE_CHANGE']/app_train['DAYS_BIRTH']\n",
    "app_train['NEW_PHONE_CHANGE_EMPLOYMENT_RATIO'] = app_train['DAYS_LAST_PHONE_CHANGE']/app_train['DAYS_EMPLOYED']\n",
    "app_train['DAYS_ID_PUBLISH_BIRTH_RAIO'] = app_train['DAYS_ID_PUBLISH']/app_train['DAYS_BIRTH']\n",
    "\n",
    "app_train['REGISTRATION_TO_NEW_PHONE_CHANGE_RATIO'] = app_train['DAYS_LAST_PHONE_CHANGE']/app_train['DAYS_REGISTRATION']\n",
    "#app_train['REGISTRATION_TO_ID_PUBLISH_RATIO'] = app_train['DAYS_ID_PUBLISH'] - app_train['DAYS_REGISTRATION']\n",
    "#app_train['REGISTRATION_TO_BIRTH_RATIO'] = app_train['DAYS_BIRTH'] - app_train['DAYS_REGISTRATION']\n",
    "\n",
    "#EXT_SOURCE son variables representan fuentes externas de información sobre el cliente, y suelen estar relacionadas con puntuaciones de riesgo crediticio generadas por \n",
    "# instituciones externas al prestamista, como burós de crédito u otras entidades que evalúan el perfil financiero de los clientes.\n",
    "#Vamos a crear variables relacionadas con EXT_SOURCE.\n",
    "\n",
    "app_train['EXT_SOURCE_SUM'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].sum(axis=1)\n",
    "app_train['EXT_SOURCE_MEAN'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "app_train['EXT_SOURCE_MEDIAN'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].median(axis=1)\n",
    "app_train['EXT_SOURCE_MAX'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].max(axis=1)\n",
    "app_train['EXT_SOURCE_MIN'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].min(axis=1)\n",
    "\n",
    "app_train['EXT_SOURCE_WEIGHTED_SUM'] = app_train['EXT_SOURCE_1']*3 + app_train['EXT_SOURCE_2']*1 + app_train['EXT_SOURCE_3']*5\n",
    "app_train['EXT_SOURCE_WEIGHTED_MEAN'] = (app_train['EXT_SOURCE_1']*3 + app_train['EXT_SOURCE_2']*1 + app_train['EXT_SOURCE_3']*5)/3\n",
    "app_train['EXT_SOURCE_PROD'] = app_train['EXT_SOURCE_1']*app_train['EXT_SOURCE_2']*app_train['EXT_SOURCE_3']\n",
    "\n",
    "app_train['RATIO_EXT_SOURCE_3_TO_REGION_POPULATION_RELATIVE'] = app_train['EXT_SOURCE_3'] / app_train['REGION_POPULATION_RELATIVE']\n",
    "\n",
    "\"\"\"\n",
    "app_train['AGE_CREDIT_RATIO'] = app_train['AGE_INT']/app_train['AMT_CREDIT']\n",
    "app_train['AGE_ANNUITY_RATIO'] = app_train['AGE_INT']/app_train['AMT_ANNUITY']\n",
    "app_train['AGE_GOODS_PRICE_RATIO'] = app_train['AGE_INT']/app_train['AMT_GOODS_PRICE']\n",
    "app_train['AGE_INCOME_RATIO'] = app_train['AGE_INT']/app_train['AMT_INCOME_TOTAL']\n",
    "\"\"\"\n",
    "app_train['AGE_CREDIT_RATIO'] = app_train['AMT_CREDIT']/app_train['DAYS_BIRTH']\n",
    "app_train['AGE_ANNUITY_RATIO'] = app_train['AMT_ANNUITY']/app_train['DAYS_BIRTH']\n",
    "app_train['AGE_GOODS_PRICE_RATIO'] = app_train['AMT_GOODS_PRICE']/app_train['DAYS_BIRTH']\n",
    "app_train['AGE_INCOME_RATIO'] = app_train['AMT_INCOME_TOTAL']/app_train['DAYS_BIRTH']\n",
    "\n",
    "\n",
    "\n",
    "#Variables a partir de sumas y diferencias\n",
    "app_train['AMT_INCOME_TOTAL_ANNUITY_SUM'] = app_train['AMT_INCOME_TOTAL'] + app_train['AMT_ANNUITY']\n",
    "app_train['AMT_GOODS_TO_INCOME_ANUITY_SUM_RATIO'] = app_train['AMT_GOODS_PRICE']/(app_train['AMT_INCOME_TOTAL_ANNUITY_SUM'] )\n",
    "app_train['CREDIT_BUREAU_TOTAL'] = app_train[['AMT_REQ_CREDIT_BUREAU_DAY', \n",
    "                                             'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    "                                             'AMT_REQ_CREDIT_BUREAU_MON',\n",
    "                                             'AMT_REQ_CREDIT_BUREAU_QRT',\n",
    "                                             'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "                                             'AMT_REQ_CREDIT_BUREAU_YEAR']].sum(axis=1)\n",
    "\n",
    "app_train['CREDIT_GOODS_DIFF'] = app_train['AMT_GOODS_PRICE'] - app_train['AMT_CREDIT']\n",
    "app_train['GOODS_ANNUITY_DIFF'] = app_train['AMT_ANNUITY'] - app_train['AMT_GOODS_PRICE']\n",
    "app_train['AMT_INCOME_TOTAL_ANNUITY_DIFF'] = app_train['AMT_INCOME_TOTAL'] - app_train['AMT_ANNUITY']\n",
    "\n",
    "#app_train['SOCIAL_OBSERVATION_TOTAL'] = app_train['OBS_30_CNT_SOCIAL_CIRCLE'] + app_train['OBS_60_CNT_SOCIAL_CIRCLE']\n",
    "#app_train['SOCIAL_DEF_TOTAL'] = app_train['DEF_30_CNT_SOCIAL_CIRCLE'] + app_train['DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "\n",
    "app_train['DIFF_OBS_30_60'] = app_train['OBS_30_CNT_SOCIAL_CIRCLE'] - app_train['OBS_60_CNT_SOCIAL_CIRCLE']\n",
    "app_train['DIFF_DEF_30_60'] = app_train['DEF_30_CNT_SOCIAL_CIRCLE'] - app_train['DEF_60_CNT_SOCIAL_CIRCLE'] \n",
    "\n",
    "app_train['LONG_EMPLOYMENT'] = np.where(app_train['DAYS_EMPLOYED'] < -2000, 1, 0)\n",
    "app_train['RETIRED'] = np.where(app_train['DAYS_BIRTH'] < -14000, 1, 0)\n",
    "\n",
    "#OTRAS\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    307511.000000\n",
      "mean          2.509507\n",
      "std           0.874544\n",
      "min           1.000000\n",
      "25%           2.000000\n",
      "50%           2.000000\n",
      "75%           3.000000\n",
      "max           5.000000\n",
      "Name: EDUCATION_LEVEL, Length: 8, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nincome_type_mapping = {\\n    'Unemployed': 0,\\n    'Student': 1,\\n    'Pensioner': 2,\\n    'Maternity leave': 3,\\n    'Working': 4,\\n    'State servant': 5,\\n    'Commercial associate': 6,\\n    'Businessman': 7,    \\n}\\napp_train['INCOME_TYPE'] = app_train['NAME_INCOME_TYPE'].map(income_type_mapping)\\n\\nfamily_mapping = {\\n    'Single / not married': 0,\\n    'Separated': 1,\\n    'Widow': 2,\\n    'Married': 3\\n}\\n\\nhousing_mapping = {\\n    'With parents': 0,\\n    'Municipal apartment': 1,\\n    'Rented apartment': 2,\\n    'House / apartment': 3\\n}\\n\\n\\n\\napp_train['FAMILY_STATUS'] = app_train['NAME_FAMILY_STATUS'].map(family_mapping)\\napp_train['HOUSING_TYPE'] = app_train['NAME_HOUSING_TYPE'].map(housing_mapping)\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "education_mapping = {\n",
    "    'No formal education': 0,\n",
    "    'Lower secondary': 1,\n",
    "    'Secondary / secondary special': 2,\n",
    "    'Incomplete higher': 3,\n",
    "    'Higher education': 4,\n",
    "    'Academic degree': 5\n",
    "}\n",
    "\n",
    "app_train['EDUCATION_LEVEL'] = app_train['NAME_EDUCATION_TYPE'].map(education_mapping)\n",
    "print(app_train['EDUCATION_LEVEL'].describe())\n",
    "\"\"\"\n",
    "income_type_mapping = {\n",
    "    'Unemployed': 0,\n",
    "    'Student': 1,\n",
    "    'Pensioner': 2,\n",
    "    'Maternity leave': 3,\n",
    "    'Working': 4,\n",
    "    'State servant': 5,\n",
    "    'Commercial associate': 6,\n",
    "    'Businessman': 7,    \n",
    "}\n",
    "app_train['INCOME_TYPE'] = app_train['NAME_INCOME_TYPE'].map(income_type_mapping)\n",
    "\n",
    "family_mapping = {\n",
    "    'Single / not married': 0,\n",
    "    'Separated': 1,\n",
    "    'Widow': 2,\n",
    "    'Married': 3\n",
    "}\n",
    "\n",
    "housing_mapping = {\n",
    "    'With parents': 0,\n",
    "    'Municipal apartment': 1,\n",
    "    'Rented apartment': 2,\n",
    "    'House / apartment': 3\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "app_train['FAMILY_STATUS'] = app_train['NAME_FAMILY_STATUS'].map(family_mapping)\n",
    "app_train['HOUSING_TYPE'] = app_train['NAME_HOUSING_TYPE'].map(housing_mapping)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2863b6823244f1840fb56a96e73f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930ff8b33533460eb7cb754b927480e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0250fc02f53a411e86f6f775c641c492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c14fa67de434df38f7146e46d1b782c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#AGREGACIONES\n",
    "agregaciones = [\n",
    "    (['CODE_GENDER', 'NAME_EDUCATION_TYPE'], [('AMT_ANNUITY', 'max'),\n",
    "                                              ('AMT_CREDIT', 'max'),\n",
    "                                              ('EXT_SOURCE_1', 'mean'),\n",
    "                                              ('EXT_SOURCE_2', 'mean'),\n",
    "                                              ('EXT_SOURCE_3', 'mean'),\n",
    "                                              ('AGE_INT', 'mean'),\n",
    "                                              ('OWN_CAR_AGE', 'max')]),\n",
    "\n",
    "    (['CODE_GENDER', 'ORGANIZATION_TYPE'], [('AMT_ANNUITY', 'mean'),\n",
    "                                            ('AMT_CREDIT', 'mean'),\n",
    "                                            ('EXT_SOURCE_1', 'mean'),\n",
    "                                            ('EXT_SOURCE_2', 'mean'),\n",
    "                                            ('EXT_SOURCE_3', 'mean')]),\n",
    "    (['CODE_GENDER', 'REG_CITY_NOT_WORK_CITY'], [('AMT_ANNUITY', 'mean'),\n",
    "                                                ('DAYS_LAST_PHONE_CHANGE', 'mean'),\n",
    "                                                ('DAYS_REGISTRATION', 'mean'),\n",
    "                                                ('CNT_CHILDREN', 'mean')]),\n",
    "\n",
    "                                              \n",
    "                                                                                                                                             \n",
    "]\n",
    "groupby_aggregate_names = []\n",
    "for groupby_cols, specs in tqdm(agregaciones):\n",
    "    group_object = app_train.groupby(groupby_cols)\n",
    "    for select, agg in tqdm(specs):\n",
    "        groupby_aggregate_name = '{}_{}_{}'.format('_'.join(groupby_cols), agg, select)\n",
    "        app_train = app_train.merge(group_object[select]\n",
    "                              .agg(agg)\n",
    "                              .reset_index()\n",
    "                              .rename(index=str,\n",
    "                                      columns={select: groupby_aggregate_name})\n",
    "                              [groupby_cols + [groupby_aggregate_name]],\n",
    "                              on=groupby_cols,\n",
    "                              how='left')\n",
    "        groupby_aggregate_names.append(groupby_aggregate_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en app_train después de la imputación:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento previo a la creación de los modelos\n",
    "# Imputación de valores nulos. Vamos a probar de momento a completar variables numéricos con la media\n",
    "# y las categóricas con la moda.\n",
    "# Como sabemos que DAYS_EMPLOYEED tiene un valor erroneo, vamos a reemplazarlo por NaN\n",
    "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "app_train['DAYS_LAST_PHONE_CHANGE'].replace({0: np.nan}, inplace = True)\n",
    "app_train['NAME_FAMILY_STATUS'].replace('Unknown', np.nan, inplace=True)\n",
    "# Reemplazar valores infinitos por NaN\n",
    "app_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# Algunas variables categóricas tienen valores como XNA, vamos a reemplazarlos por NaN\n",
    "def reemplazar_xna_por_nan(app_train):\n",
    "    variables_categoricas = app_train.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in variables_categoricas:\n",
    "        app_train[col] = app_train[col].replace('XNA', np.nan)\n",
    "    return app_train\n",
    "app_train_noxna = reemplazar_xna_por_nan(app_train)\n",
    "\n",
    "app_train = reemplazar_xna_por_nan(app_train)\n",
    "def imputar_valores_nulos(app_train):\n",
    "    variables_continuas = app_train.select_dtypes(include=['int64', 'float64']).columns \n",
    "    for col in variables_continuas:\n",
    "        app_train[col] = app_train[col].fillna(app_train[col].mean())\n",
    "    variables_categoricas = app_train.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in variables_categoricas:\n",
    "        app_train[col] = app_train[col].fillna(app_train[col].mode()[0])\n",
    "\n",
    "    return app_train\n",
    "\n",
    "app_train_nonan = imputar_valores_nulos(app_train_noxna)\n",
    "#Comprobamos que no haya valores nulos\n",
    "#print(app_train['DAYS_BIRTH'].unique())\n",
    "#print(app_train['DAYS_BIRTH'].isnull().sum())\n",
    "#print(app_train['DAYS_BIRTH'].dtype)\n",
    "\n",
    "# Verificar si hay valores nulos en app_train después de la imputación\n",
    "print(\"Valores nulos en app_train después de la imputación:\")\n",
    "print(app_train_nonan.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en app_train después de la imputación:\n",
      "0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 307511 entries, 0 to 307510\n",
      "Columns: 720 entries, TARGET to LATEST_CREDIT_ACTIVE_CAT_(BUREAU)_Sold\n",
      "dtypes: bool(148), float64(523), int32(8), int64(41)\n",
      "memory usage: 1.3 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Selección de variables categóricas\n",
    "variables_categoricas = app_train.select_dtypes(include=['object', 'category']).columns\n",
    "variables_categoricas_binarias = [col for col in variables_categoricas if app_train[col].nunique() == 2]\n",
    "variables_categoricas_no_binarias = [col for col in variables_categoricas if app_train[col].nunique() > 2]\n",
    "\n",
    "# Diccionario para almacenar los LabelEncoders\n",
    "label_encoders_binarias = {}\n",
    "\n",
    "# Función para ajustar el LabelEncoder a las variables binarias\n",
    "def label_encoding_binarias_fit(app_train):\n",
    "    for col in variables_categoricas_binarias:\n",
    "        Encoder = LabelEncoder()\n",
    "        Encoder.fit(app_train[col])\n",
    "        label_encoders_binarias[col] = Encoder\n",
    "\n",
    "# Función para transformar las variables binarias utilizando los LabelEncoders ajustados\n",
    "def label_encoding_binarias_transform(data):\n",
    "    data = data.copy()  # Evitar modificar el original\n",
    "    for col in variables_categoricas_binarias:\n",
    "        if col in label_encoders_binarias:\n",
    "            data[col] = label_encoders_binarias[col].transform(data[col])\n",
    "    return data\n",
    "\n",
    "# Función para aplicar One-Hot Encoding a las variables no binarias\n",
    "def one_hot_encoding_no_binarias(data):\n",
    "    data = data.copy()  # Evitar modificar el original\n",
    "    data = pd.get_dummies(data, columns=variables_categoricas_no_binarias)\n",
    "    return data\n",
    "\n",
    "# Ajustar los LabelEncoders al conjunto de entrenamiento\n",
    "label_encoding_binarias_fit(app_train)\n",
    "\n",
    "# Transformar el conjunto de entrenamiento\n",
    "app_train_nonan_le = label_encoding_binarias_transform(app_train)\n",
    "\n",
    "# Aplicar One-Hot Encoding al conjunto de entrenamiento\n",
    "app_train_nonan_le_oh = one_hot_encoding_no_binarias(app_train_nonan_le)\n",
    "\n",
    "print(\"Valores nulos en app_train después de la imputación:\")\n",
    "print(app_train_nonan_le_oh.isnull().sum().sum())\n",
    "print(app_train_nonan_le_oh.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en app_train después de la imputación:\n",
      "0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 307511 entries, 0 to 307510\n",
      "Columns: 720 entries, TARGET to LATEST_CREDIT_ACTIVE_CAT_(BUREAU)_Sold\n",
      "dtypes: bool(148), float64(530), int32(8), int64(34)\n",
      "memory usage: 1.3 GB\n",
      "None\n",
      "TARGET                                                     1\n",
      "NAME_CONTRACT_TYPE                                         1\n",
      "CODE_GENDER                                                1\n",
      "FLAG_OWN_CAR                                               1\n",
      "FLAG_OWN_REALTY                                            1\n",
      "                                                        ... \n",
      "LATEST_CREDIT_TYPE_CAT_(BUREAU)_Unknown type of loan    True\n",
      "LATEST_CREDIT_ACTIVE_CAT_(BUREAU)_Active                True\n",
      "LATEST_CREDIT_ACTIVE_CAT_(BUREAU)_Bad debt              True\n",
      "LATEST_CREDIT_ACTIVE_CAT_(BUREAU)_Closed                True\n",
      "LATEST_CREDIT_ACTIVE_CAT_(BUREAU)_Sold                  True\n",
      "Length: 720, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Identificar las variables continuas que necesitan normalización\n",
    "variables_continuas = app_train_nonan_le_oh.select_dtypes(include=['int64', 'float64']).columns\n",
    "variables_normalizadas = [col for col in variables_continuas if\n",
    "                          app_train_nonan_le_oh[col].min() >= 0 and app_train_nonan_le_oh[col].max() <= 1]\n",
    "variables_NO_normalizadas = [col for col in variables_continuas if col not in variables_normalizadas]\n",
    "\n",
    "# Diccionario para almacenar los escaladores ajustados\n",
    "scalers = {}\n",
    "\n",
    "# Función para ajustar el escalador a las variables no normalizadas\n",
    "def normalizacion_fit(data, cols, scaler_type='minmax'):\n",
    "    for col in cols:\n",
    "        if scaler_type == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaler_type == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(\"Tipo de escalador no reconocido. Usa 'minmax' o 'robust'.\")\n",
    "        scaler.fit(data[[col]])\n",
    "        scalers[col] = scaler\n",
    "\n",
    "# Función para transformar las variables utilizando los escaladores ajustados\n",
    "def normalizacion_transform(data, cols):\n",
    "    data = data.copy()  # Evitar modificar el original\n",
    "    for col in cols:\n",
    "        if col in scalers:\n",
    "            data[col] = scalers[col].transform(data[[col]])\n",
    "    return data\n",
    "\n",
    "# Ajustar los escaladores a las variables no normalizadas en el conjunto de entrenamiento\n",
    "normalizacion_fit(app_train_nonan_le_oh, variables_NO_normalizadas, scaler_type='minmax')\n",
    "\n",
    "# Transformar el conjunto de entrenamiento\n",
    "app_train_nonan_le_oh_norm = normalizacion_transform(app_train_nonan_le_oh, variables_NO_normalizadas)\n",
    "\n",
    "# Verificar los valores nulos y la información del DataFrame transformado\n",
    "print(\"Valores nulos en app_train después de la imputación:\")\n",
    "print(app_train_nonan_le_oh_norm.isnull().sum().sum())\n",
    "print(app_train_nonan_le_oh_norm.info())\n",
    "print(app_train_nonan_le_oh_norm.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportamos el csv tras todo el preprocesamiento\n",
    "app_train_nonan_le_oh_norm.to_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/application_train_preprocesado_definitivo_v3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
