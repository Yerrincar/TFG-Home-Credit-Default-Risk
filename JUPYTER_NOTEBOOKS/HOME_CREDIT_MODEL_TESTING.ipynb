{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n",
      "Directorio actual: /home/yeray/TFG-Home-Credit-Default-Risk/JUPYTER_NOTEBOOKS\n",
      "(307511, 1323)\n"
     ]
    }
   ],
   "source": [
    "#Celda para librerías\n",
    "import sklearn as sk\n",
    "\n",
    "\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# LightGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(np.__version__)\n",
    "from ydata_profiling import ProfileReport\n",
    "import os\n",
    "print(\"Directorio actual:\", os.getcwd())\n",
    "\n",
    "# Have all columns appear when dataframes are displayed.\n",
    "pd.set_option('display.max_columns', None) \n",
    "# Have 100 rows appear when a dataframe is displayed\n",
    "pd.set_option('display.max_rows', 500)\n",
    "# Display dimensions whenever a dataframe is printed out.\n",
    "pd.set_option('display.show_dimensions', True)\n",
    "\n",
    "#Importando los datos\n",
    "\n",
    "#app_train_def = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/application_train_preprocesado_definitivo_v1.csv')\n",
    "#app_train_def_2 = pd.read_csv(r'/home/yeray/TFG-Home-Credit-Default-Risk/JUPYTER_NOTEBOOKS/DATA/application_train_preprocesado_definitivo_v2.csv')\n",
    "#app_train_def_3 = pd.read_csv(r'/home/yeray/TFG-Home-Credit-Default-Risk/JUPYTER_NOTEBOOKS/DATA/application_train_preprocesado_definitivo_v3.csv')\n",
    "#app_train_def_4 = pd.read_csv(r'/home/yeray/TFG-Home-Credit-Default-Risk/JUPYTER_NOTEBOOKS/DATA/application_train_preprocesado_definitivo_v4.csv')\n",
    "#app_train_def_5 = pd.read_csv(r'/home/yeray/TFG-Home-Credit-Default-Risk/JUPYTER_NOTEBOOKS/DATA/application_train_preprocesado_definitivo_v5.csv')\n",
    "#app_train_def_6 = pd.read_csv(r'/home/yeray/TFG-Home-Credit-Default-Risk/JUPYTER_NOTEBOOKS/DATA/application_train_preprocesado_definitivo_v6.csv')\n",
    "app_train_def_7 = pd.read_csv(r'/home/yeray/TFG-Home-Credit-Default-Risk/JUPYTER_NOTEBOOKS/DATA/application_train_preprocesado_definitivo_v7.csv')\n",
    "#app_train_def_7_v2 = pd.read_csv(r'/home/yeray/TFG-Home-Credit-Default-Risk/JUPYTER_NOTEBOOKS/DATA/app_def_v7_menor_dimension.csv')\n",
    "#print(app_train_def_5.shape)\n",
    "#print(app_train_def_6.shape)\n",
    "print(app_train_def_7.shape)\n",
    "#print(app_train_def_7_v2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train y Test\n",
    "X = app_train_def_7.drop(columns = ['TARGET'])\n",
    "y = app_train_def_7['TARGET']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train.columns = X_train.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "X_test.columns = X_test.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos espacio de hiperparámetros para LightGBM\n",
    "space_lightGBM = {\n",
    "    'boosting_type': 'gbdt',  # Fijo\n",
    "    'objective': 'binary',   # Fijo\n",
    "    'metric': 'auc',         # Fijo\n",
    "    ##AJUSTAR MAX DEPTH\n",
    "    'num_leaves': hp.quniform('num_leaves', 25, 45, 1), \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.03),  \n",
    "    'min_child_samples': hp.quniform('min_child_samples', 70, 100, 1),  \n",
    "    'min_child_weight': hp.uniform('min_child_weight', 0.001, 0.02),\n",
    "    'min_gain_to_split': hp.uniform('min_gain_to_split', 0.001, 0.1),\n",
    "    'max_bin': hp.quniform('max_bin', 220, 350, 10),\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.15, 0.5),\n",
    "    'max_depth': hp.choice('max_depth', [-1, 3, 5]), \n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1.0), #No admite valores mayores a 1.0\n",
    "    'bagging_freq': hp.quniform('bagging_freq', 7, 15, 1),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 70),  \n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 0.2),  \n",
    "    \n",
    "    'scale_pos_weight': 1,  # Fijo\n",
    "    'is_unbalance': False  # Fijo\n",
    "}\n",
    "\n",
    "Trials_lightGBM_Ligero = Trials()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 400\n",
    "\n",
    "def F_OPT(params):\n",
    "    params['max_bin'] = int(params['max_bin'])\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['min_child_samples'] = int(params['min_child_samples'])\n",
    "    params['bagging_freq'] = int(params['bagging_freq'])\n",
    "    params['min_child_samples'] = int(params['min_child_samples'])\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=n_iter,\n",
    "        boosting_type=params['boosting_type'],\n",
    "        objective=params['objective'],\n",
    "        metric=params['metric'],\n",
    "        num_leaves=params['num_leaves'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        min_child_samples=params['min_child_samples'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        min_gain_to_split=params['min_gain_to_split'],\n",
    "        max_bin=params['max_bin'],\n",
    "        feature_fraction=params['feature_fraction'],\n",
    "        max_depth=params['max_depth'],\n",
    "        bagging_fraction=params['bagging_fraction'],\n",
    "        bagging_freq=params['bagging_freq'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        is_unbalance=params['is_unbalance'],\n",
    "        random_state=42)\n",
    "    \n",
    "    cv_Strat = StratifiedKFold(n_splits=5)\n",
    "    auc_lightGBM_Ligero = cross_val_score(model, X_train, y_train, cv=cv_Strat, scoring='roc_auc').mean()\n",
    "    return {'loss': -auc_lightGBM_Ligero, 'status': STATUS_OK}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "best_lightGBM_Ligero = fmin(fn=F_OPT,\n",
    "                            space=space_lightGBM,\n",
    "                            algo=tpe.suggest,\n",
    "                            max_evals=50,\n",
    "                            trials=Trials_lightGBM_Ligero,\n",
    "                            rstate= np.random.default_rng(42)\n",
    ")\n",
    "\n",
    "best_lightGBM_Ligero['num_leaves'] = int(best_lightGBM_Ligero['num_leaves'])\n",
    "best_lightGBM_Ligero['max_bin'] = int(best_lightGBM_Ligero['max_bin'])\n",
    "best_lightGBM_Ligero['min_child_samples'] = int(best_lightGBM_Ligero['min_child_samples'])\n",
    "best_lightGBM_Ligero['bagging_freq'] = int(best_lightGBM_Ligero['bagging_freq'])\n",
    "best_lightGBM_Ligero['min_child_samples'] = int(best_lightGBM_Ligero['min_child_samples'])\n",
    "\n",
    "#print(best_lightGBM_Ligero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#with open(\"Trials_LightGBM_Ligero_v7_server_50_evals_rango_ampliado.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(Trials_lightGBM_Ligero, f)\n",
    "with open(\"/home/yeray/TFG-Home-Credit-Default-Risk/JUPYTER_NOTEBOOKS/Trials/Trials_LightGBM_Ligero_v7_server_50_evals_rango_ampliado.pkl\", \"rb\") as f:\n",
    "    Trials_lightGBM_Ligero = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos los resultados de los trials \n",
    "from tabulate import tabulate\n",
    "\n",
    "def trials_to_dataframe(trials):\n",
    "    \"\"\"\n",
    "    Convierte la info de 'trials' de Hyperopt en un DataFrame \n",
    "    con columnas relevantes (loss, hiperparámetros, etc.).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for trial in trials.trials:\n",
    "        # trial['result']['loss'] -> la métrica\n",
    "        # trial['misc']['vals'] -> diccionario de hiperparámetros propuestos\n",
    "        loss = trial['result']['loss']\n",
    "        vals = trial['misc']['vals']\n",
    "        \n",
    "        # Convertir vals a algo \"plano\"\n",
    "        row = {**vals}\n",
    "        row['loss'] = loss\n",
    "        rows.append(row)\n",
    "    \n",
    "    # df con columnas = keys (vals + 'loss')\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # A veces los hps que eran hp.quniform se quedan en listas => df['num_leaves'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "    for c in df.columns:\n",
    "        df[c] = df[c].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Para usarlo:\n",
    "results_df = trials_to_dataframe(Trials_lightGBM_Ligero)\n",
    "# Ordenamos por 'loss' asc => AUC mayor es 'loss' menor\n",
    "results_df_sorted = results_df.sort_values(by='loss', ascending=True)\n",
    "best_10 = results_df_sorted.head(10)\n",
    "print(tabulate(best_10, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a realizar una selección de variables previa a una optimización más completa y una búsqueda SS\n",
    "# más exhaustiva de hiperparámetros. Para ello, vamos a utilizar Permutation Importance.\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Extraer el mejor trial en forma de diccionario\n",
    "best_trial = results_df_sorted.iloc[0].to_dict()\n",
    "\n",
    "for key in ['num_leaves', 'max_bin', 'min_child_samples', 'bagging_freq', 'max_depth']:\n",
    "    best_trial[key] = int(best_trial[key])\n",
    "# Eliminar la clave 'loss' ya que no es un hiperparámetro para el modelo\n",
    "best_trial.pop('loss', None)\n",
    "\n",
    "# Si es necesario, puedes asegurarte de que los valores sean del tipo correcto,\n",
    "# aunque en nuestro proceso ya se han aplanado.\n",
    "\n",
    "# Crear el modelo utilizando el operador ** para desempaquetar el diccionario de hiperparámetros\n",
    "model_ligero = LGBMClassifier(\n",
    "    n_estimators=n_iter,          # Puedes dejar este parámetro fijo o incluirlo en best_trial\n",
    "    boosting_type='gbdt',         # Fijo\n",
    "    objective='binary',           # Fijo\n",
    "    metric='auc',                 # Fijo\n",
    "    scale_pos_weight=1,           # Fijo\n",
    "    is_unbalance=False,           # Fijo\n",
    "    random_state=42,              # Fijo\n",
    "    **best_trial                 # Desempaqueta los hiperparámetros encontrados\n",
    ")\n",
    "\n",
    "# Ajustar el modelo\n",
    "model_ligero.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pi = permutation_importance(\n",
    "    estimator=model_ligero,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    scoring='roc_auc',\n",
    "    n_repeats=20,\n",
    "    random_state=42\n",
    "    #Probar n jobs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#with open(\"pi_lightGBM_Ligero_v7_server_20_reapeats.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(pi, f)\n",
    "#TFG-Home-Credit-Default-Risk/JUPYTER_NOTEBOOKS/Permutation_Pickle/pi_lightGBM_Ligero_v7_server_20_reapeats.pkl\n",
    "with open(\"/home/yeray/TFG-Home-Credit-Default-Risk/JUPYTER_NOTEBOOKS/Permutation_Pickle/pi_lightGBM_Ligero_v7_server_20_reapeats.pkl\", \"rb\") as f:\n",
    "    pi = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "features = X_train.columns\n",
    "importances_mean = pi.importances_mean\n",
    "importances_std = pi.importances_std\n",
    "\n",
    "#Lo unimos en un DF\n",
    "\n",
    "df_pi = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance_mean': importances_mean,\n",
    "    'importance_std': importances_std\n",
    "})\n",
    "df_pi.to_csv(\"Permutation_Importance_LightGBM_Ligero_v7_server_20_repeats.csv\", index=False)\n",
    "\n",
    "df_pi_sorted = df_pi.sort_values(by='importance_mean', ascending=False)\n",
    "print(tabulate(df_pi_sorted, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_features_pi = df_pi_sorted[df_pi_sorted['importance_mean'] > 0]\n",
    "subset_features= subset_features_pi['feature'].tolist()\n",
    "X_train_subset = X_train[subset_features]\n",
    "y_train_subset = y_train\n",
    "X_test_subset = X_test[subset_features]\n",
    "y_test_subset = y_test\n",
    "#print(X_train_subset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "space_lightGBM_completo = {\n",
    "    'boosting_type': 'gbdt',  # Fijo\n",
    "    'objective': 'binary',   # Fijo\n",
    "    'metric': 'auc',         # Fijo\n",
    "    #Ajustamos los HP para centrarlos y reducir el rango a los resultados obtenidos anteriormente\n",
    "    'num_leaves': hp.quniform('num_leaves', 35, 45, 1), \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.015, 0.03),  \n",
    "    'min_child_samples': hp.quniform('min_child_samples', 80, 110, 1),  \n",
    "    'min_child_weight': hp.uniform('min_child_weight', 0.01, 0.015),\n",
    "    'min_gain_to_split': hp.uniform('min_gain_to_split', 0.02, 0.1),\n",
    "    'max_bin': hp.quniform('max_bin', 250, 300, 5),\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.1, 0.4),\n",
    "    'max_depth': hp.choice('max_depth', [-1, 3]), \n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.6, 1.0),\n",
    "    'bagging_freq': hp.quniform('bagging_freq', 10, 18, 1),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 50),  \n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.05, 0.15),  \n",
    "    \n",
    "    'scale_pos_weight': 1,  # Fijo\n",
    "    'is_unbalance': False  # Fijo\n",
    "}\n",
    "\"\"\"\n",
    "## PRUEBAS PARA CONTROLAR LOS PROBLEMAS CON LA PROFUNDIDAD\n",
    "# Tras varias pruebas e investigaciones, voy a probar primero ciertas cosas que me interesan en el csv reducido y luego al completo\n",
    "space_lightGBM_completo = {\n",
    "    'boosting_type': 'gbdt',          # Fijo\n",
    "    'objective': 'binary',            # Fijo\n",
    "    'metric': 'auc',                  # Fijo\n",
    "    # Ajustamos el rango de num_leaves para permitir mayor flexibilidad, aunque puede que nos provoque overfitting, así que vamos a probar y vamos viendo\n",
    "    'num_leaves': hp.quniform('num_leaves', 35, 45, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.015, 0.018),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 100, 110, 1),\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 0.01, 0.015),\n",
    "    'min_gain_to_split': hp.uniform('min_gain_to_split', 0.02, 0.03),\n",
    "    'max_bin': hp.quniform('max_bin', 290, 310, 5),\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.1, 0.15),\n",
    "    'n_estimators': hp.uniform('n_estimators', 1900,2200,100),\n",
    "    # Permitimos explorar tanto árboles sin límite como con profundidad controlada\n",
    "    'max_depth': hp.choice('max_depth', [-1]),\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.85, 0.9),\n",
    "    'bagging_freq': hp.quniform('bagging_freq', 15, 18, 1),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 30, 40),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.09, 0.11),\n",
    "    'n_estimators': hp.quniform('n_estimators', 2000, 2300, 100),\n",
    "    'scale_pos_weight': 1,            # Fijo\n",
    "    'is_unbalance': False             # Fijo\n",
    "}\n",
    "Trials_lightGBM_Completo = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 2000\n",
    "#Vamos a probar ahora a realizar esa búsqueda más extensa pero con seleccionando las variables que parecen tener mayor releveancia\n",
    "def F_OPT_Completa(params):\n",
    "    params['max_bin'] = int(params['max_bin'])\n",
    "    params['min_child_samples'] = int(params['min_child_samples'])\n",
    "    params['bagging_freq'] = int(params['bagging_freq'])\n",
    "    params['min_child_samples'] = int(params['min_child_samples'])\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "\n",
    "    depth = params['max_depth']\n",
    "    num_leaves = params['num_leaves']\n",
    "\n",
    "    # Si max_depth es positivo (por ej. 4, 5, 6...), forzamos la regla:\n",
    "    # num_leaves <= 2^depth\n",
    "    \"\"\"\n",
    "    if depth > 0:\n",
    "        max_leaves_allowed = 2 ** depth\n",
    "        if num_leaves > max_leaves_allowed:\n",
    "            num_leaves = max_leaves_allowed\n",
    "\n",
    "    # Asignamos de vuelta\n",
    "    params['max_depth'] = depth\n",
    "    params['num_leaves'] = num_leaves\n",
    "    \"\"\"\n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        #early_stopping_round=50,\n",
    "        boosting_type=params['boosting_type'],\n",
    "        objective=params['objective'],\n",
    "        metric=params['metric'],\n",
    "        num_leaves=params['num_leaves'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        min_child_samples=params['min_child_samples'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        min_gain_to_split=params['min_gain_to_split'],\n",
    "        max_bin=params['max_bin'],\n",
    "        feature_fraction=params['feature_fraction'],\n",
    "        max_depth=params['max_depth'],\n",
    "        bagging_fraction=params['bagging_fraction'],\n",
    "        bagging_freq=params['bagging_freq'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        is_unbalance=params['is_unbalance'],\n",
    "        random_state=42,\n",
    "        n_jobs=20\n",
    "        )\n",
    "    \n",
    "    cv_Strat = StratifiedKFold(n_splits=5)\n",
    "    auc_lightGBM_Completo = cross_val_score(model, X_train, y_train, cv=cv_Strat, scoring='roc_auc').mean()\n",
    "    return {'loss': -auc_lightGBM_Completo, 'status': STATUS_OK}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "best_lightGBM_Completo = fmin(fn=F_OPT_Completa,\n",
    "                            space=space_lightGBM_completo,\n",
    "                            algo=tpe.suggest,\n",
    "                            max_evals=50,\n",
    "                            trials=Trials_lightGBM_Completo,\n",
    "                            rstate= np.random.default_rng(42)\n",
    ")\n",
    "best_lightGBM_Completo['max_bin'] = int(best_lightGBM_Completo['max_bin'])\n",
    "best_lightGBM_Completo['min_child_samples'] = int(best_lightGBM_Completo['min_child_samples'])\n",
    "best_lightGBM_Completo['bagging_freq'] = int(best_lightGBM_Completo['bagging_freq'])\n",
    "best_lightGBM_Completo['min_child_samples'] = int(best_lightGBM_Completo['min_child_samples'])\n",
    "best_lightGBM_Completo['num_leaves'] = int(best_lightGBM_Completo['num_leaves'])\n",
    "best_lightGBM_Completo['n_estimators'] = int(best_lightGBM_Completo['n_estimators'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"Trials_LightGBM_Completo_v7_ULTIMA_PRUEBA\", \"wb\") as f:\n",
    "    pickle.dump(Trials_lightGBM_Completo, f)\n",
    "\n",
    "#with open(\"/home/yeray/TFG-Home-Credit-Default-Risk/JUPYTER_NOTEBOOKS/Trials/Trials_LightGBM_Completo_v7_server_100_evals_csv_completo.pkl\", \"rb\") as f:\n",
    "#    Trials_lightGBM_Completo = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------------+--------------------+-----------------+-----------+-------------+---------------------+--------------------+---------------------+----------------+--------------+-------------+--------------+-----------+\n",
      "|    |   bagging_fraction |   bagging_freq |   feature_fraction |   learning_rate |   max_bin |   max_depth |   min_child_samples |   min_child_weight |   min_gain_to_split |   n_estimators |   num_leaves |   reg_alpha |   reg_lambda |      loss |\n",
      "|----+--------------------+----------------+--------------------+-----------------+-----------+-------------+---------------------+--------------------+---------------------+----------------+--------------+-------------+--------------+-----------|\n",
      "| 32 |           0.874354 |             17 |           0.111451 |       0.0153463 |       300 |           0 |                 104 |          0.0121249 |           0.023562  |           2200 |           35 |   0.0982616 |      31.6216 | -0.794787 |\n",
      "|  4 |           0.890404 |             17 |           0.108201 |       0.017035  |       305 |           0 |                 105 |          0.0120178 |           0.0233817 |           2100 |           38 |   0.0919329 |      35.029  | -0.794637 |\n",
      "| 41 |           0.886951 |             16 |           0.118349 |       0.0160326 |       295 |           0 |                 103 |          0.014436  |           0.025522  |           2300 |           36 |   0.0916304 |      30.9986 | -0.794617 |\n",
      "| 15 |           0.852946 |             18 |           0.132523 |       0.0154746 |       310 |           0 |                 106 |          0.0105278 |           0.0285726 |           2100 |           37 |   0.109755  |      38.7525 | -0.794609 |\n",
      "| 23 |           0.865215 |             17 |           0.134882 |       0.0168874 |       305 |           0 |                 100 |          0.0100048 |           0.0287086 |           2000 |           37 |   0.0967948 |      39.7694 | -0.794595 |\n",
      "| 33 |           0.874161 |             17 |           0.109742 |       0.0173791 |       300 |           0 |                 101 |          0.0119947 |           0.0236337 |           2200 |           35 |   0.0951194 |      31.4042 | -0.794567 |\n",
      "| 49 |           0.897651 |             18 |           0.127144 |       0.0154058 |       290 |           0 |                 105 |          0.0140004 |           0.0238886 |           2200 |           37 |   0.0930753 |      33.5333 | -0.794527 |\n",
      "| 45 |           0.88232  |             17 |           0.10461  |       0.016767  |       305 |           0 |                 104 |          0.0119419 |           0.0215891 |           2100 |           36 |   0.095878  |      34.4741 | -0.79451  |\n",
      "| 31 |           0.895355 |             18 |           0.119643 |       0.0150223 |       310 |           0 |                 107 |          0.0143789 |           0.02828   |           2000 |           40 |   0.108576  |      38.0581 | -0.794485 |\n",
      "| 13 |           0.872215 |             15 |           0.128246 |       0.0159155 |       305 |           0 |                 106 |          0.0108614 |           0.0275875 |           2300 |           36 |   0.099343  |      30.6742 | -0.794439 |\n",
      "+----+--------------------+----------------+--------------------+-----------------+-----------+-------------+---------------------+--------------------+---------------------+----------------+--------------+-------------+--------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "#Mostramos los resultados de los trials \n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "def trials_to_dataframe(trials):\n",
    "    rows = []\n",
    "    for trial in trials.trials:\n",
    "        if 'loss' not in trial['result']:\n",
    "            # saltar este trial\n",
    "            continue\n",
    "\n",
    "        loss = trial['result']['loss']\n",
    "        vals = trial['misc']['vals']\n",
    "        \n",
    "        row = {**vals}\n",
    "        row['loss'] = loss\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    for c in df.columns:\n",
    "        df[c] = df[c].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Para usarlo:\n",
    "results_df = trials_to_dataframe(Trials_lightGBM_Completo)\n",
    "# Ordenamos por 'loss' asc => AUC mayor es 'loss' menor\n",
    "results_df_sorted = results_df.sort_values(by='loss', ascending=True)\n",
    "best_10 = results_df_sorted.head(10)\n",
    "print(tabulate(best_10, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_fraction': 0.8743538556507691, 'bagging_freq': 17, 'feature_fraction': 0.11145097371066257, 'learning_rate': 0.015346271469888557, 'max_bin': 300, 'max_depth': 0, 'min_child_samples': 104, 'min_child_weight': 0.012124933383198718, 'min_gain_to_split': 0.023561958354995503, 'n_estimators': 2200, 'num_leaves': 35, 'reg_alpha': 0.09826158385027922, 'reg_lambda': 31.621566021026435}\n",
      "[LightGBM] [Warning] bagging_freq is set=17, subsample_freq=0 will be ignored. Current value: bagging_freq=17\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.023561958354995503, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.023561958354995503\n",
      "[LightGBM] [Warning] feature_fraction is set=0.11145097371066257, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.11145097371066257\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8743538556507691, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8743538556507691\n",
      "[LightGBM] [Warning] bagging_freq is set=17, subsample_freq=0 will be ignored. Current value: bagging_freq=17\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.023561958354995503, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.023561958354995503\n",
      "[LightGBM] [Warning] feature_fraction is set=0.11145097371066257, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.11145097371066257\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8743538556507691, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8743538556507691\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.037448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 265339\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 1251\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432482\n",
      "[LightGBM] [Info] Start training from score -2.432482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOptimización Completa:\\n|    |   bagging_fraction |   bagging_freq |   feature_fraction |   learning_rate |   max_bin |   max_depth |   min_child_samples |   min_child_weight |   min_gain_to_split |   num_leaves |   reg_alpha |   reg_lambda |      loss |\\n|----+--------------------+----------------+--------------------+-----------------+-----------+-------------+---------------------+--------------------+---------------------+--------------+-------------+--------------+-----------|\\n| 69 |           0.891186 |             16 |           0.106319 |       0.0173115 |       300 |           0 |                 101 |          0.0137934 |           0.0269529 |           41 |   0.0906638 |      31.1379 | -0.794675\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extraer el mejor trial en forma de diccionario\n",
    "\n",
    "best_trial = results_df_sorted.iloc[0].to_dict()\n",
    "\n",
    "for key in ['num_leaves', 'max_bin', 'min_child_samples', 'bagging_freq', 'max_depth', 'n_estimators']:\n",
    "    best_trial[key] = int(best_trial[key])\n",
    "# Eliminar la clave 'loss' ya que no es un hiperparámetro para el modelo\n",
    "best_trial.pop('loss', None)\n",
    "\n",
    "print(best_trial)\n",
    "\n",
    "# Crear el modelo utilizando el operador ** para desempaquetar el diccionario de hiperparámetros\n",
    "final_model_lgb = LGBMClassifier(        \n",
    "    boosting_type='gbdt',         # Fijo\n",
    "    objective='binary',           # Fijo\n",
    "    metric='auc',                 # Fijo\n",
    "    scale_pos_weight=1,           # Fijo\n",
    "    is_unbalance=False,           # Fijo\n",
    "    random_state=42,              # Fijo\n",
    "    **best_trial\n",
    ")\n",
    "\n",
    "\n",
    "final_model_lgb.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Optimización Completa:\n",
    "|    |   bagging_fraction |   bagging_freq |   feature_fraction |   learning_rate |   max_bin |   max_depth |   min_child_samples |   min_child_weight |   min_gain_to_split |   num_leaves |   reg_alpha |   reg_lambda |      loss |\n",
    "|----+--------------------+----------------+--------------------+-----------------+-----------+-------------+---------------------+--------------------+---------------------+--------------+-------------+--------------+-----------|\n",
    "| 69 |           0.891186 |             16 |           0.106319 |       0.0173115 |       300 |           0 |                 101 |          0.0137934 |           0.0269529 |           41 |   0.0906638 |      31.1379 | -0.794675\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=17, subsample_freq=0 will be ignored. Current value: bagging_freq=17\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.023561958354995503, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.023561958354995503\n",
      "[LightGBM] [Warning] feature_fraction is set=0.11145097371066257, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.11145097371066257\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8743538556507691, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8743538556507691\n",
      "AUC en test: 0.7980380652469227\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = final_model_lgb.predict_proba(X_test)[:, 1]\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_test = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'AUC en test: {auc_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-AUC solo con app_train:\n",
    "0.7687757918575168\n",
    "\n",
    "-AUC añadiendo bureau.csv con threshhold de 0.00015:\n",
    "0.7723138876162285\n",
    "\n",
    "-AUC con threshold de 0.0001 y 40 features:\n",
    "0.7741547798044517\n",
    "\n",
    "-AUC con threshold de 0 y ~80 features:\n",
    "0.777795871108371\n",
    "\n",
    "Comienzo de pruebas en server\n",
    "-Realizando Trials con 50 evaluaciones con todo el csv al completo el mejor resultado es de:\n",
    "\n",
    "| 36 |           0.954169 |              4 |           0.316205 |       0.0289061 |       260 |           0 |                  62 |         0.00154331 |           0.0661147 |           39 |  0.134166   |      58.288  | -0.775321 |\n",
    "\n",
    "Si probamos esta misma configuración sobre el test, es decir, HP con 50 evals y todos los features obtenemos: \n",
    "-AUC en test: 0.7807744950085171\n",
    "\n",
    "-Realizando Trials con  100 evaluaciones con un rango más acotado de HP acorde a las pruebas anteriores, además de realizando una selección de variables con un threshold >0 (443 features) obtenemos:\n",
    "\n",
    "|    |   bagging_fraction |   bagging_freq |   feature_fraction |   learning_rate |   max_bin |   max_depth |   min_child_samples |   min_child_weight |   min_gain_to_split |   num_leaves |   reg_alpha |   reg_lambda |      loss |\r\n",
    "|----+--------------------+----------------+--------------------+-----------------+-----------+-------------+---------------------+--------------------+---------------------+--------------+-------------+--------------+-----------|\r\n",
    "| 66 |           0.947598 |              5 |           0.256301 |       0.016433  |       270 |           0 |                  61 |         0.00531868 |           0.0774823 |           37 |   0.053589  |      37.9963 | -0.779018\n",
    "\n",
    "Si probamos esta configuración sobre el test obtenemos:\n",
    "-AUC en test: 0.7806464630531091 ---- Podemos observar que tenemos un resultado minimamente peor, pero puede deberse a fluctuaciones aleatorios, haber excluido alguena feature un poco útil... Aunque al ser una diferencia de ±0.0001–0.0002 de AUC se puede considerar que se encuentra dentro del margen de ruido.\n",
    "\n",
    "-AUC en test: 0.7815232040819751 He puesto nstimators en 2000 en vez de 1000, que se me había pasado cambiarlo\n",
    "\n",
    "Si añadimos los datos de la tabla bureau_balance y realizamos una optimización inicial (50), seleccionamos variables con threshold >0 (478 variables) y luego una optimizacón final (100), obtenemos un resultado de:\n",
    "-AUC en test: 0.780960248215274\n",
    "\n",
    "Si realizamos las pruebas sobre todo el csv sin realizar una selección de variables previa obtenemos:\n",
    "\n",
    "| 65 |           0.946932 |             10 |           0.202777 |       0.0172655 |       250 |           0 |                  86 |         0.00728436 |           0.0451036 |           33 |   0.0558485 |      35.8496 | -0.779158 |\n",
    "\n",
    "Y si realizamos las pruebas sobre el test:\n",
    "-AUC en test: 0.7813628078996644\n",
    "\n",
    "Tras añadir la tabla previous_application obtenemos con una primera optimización el mejor resultado obtenido es:\n",
    "\n",
    "|    |   bagging_fraction |   bagging_freq |   feature_fraction |   learning_rate |   max_bin |   max_depth |   min_child_samples |   min_child_weight |   min_gain_to_split |   num_leaves |   reg_alpha |   reg_lambda |      loss |\r\n",
    "|----+--------------------+----------------+--------------------+-----------------+-----------+-------------+---------------------+--------------------+---------------------+--------------+-------------+--------------+-----------|\r\n",
    "| 39 |           0.758684 |             12 |           0.166488 |       0.0289361 |       250 |           0 |                  96 |          0.0115629 |          0.0289674  |           40 |   0.108431  |      31.6312 | -0.783675\n",
    "\n",
    "Vamos a realizar diferentes pruebas con el número de variables para ver como varían los resulta\n",
    "\n",
    "En primer lugar, realizando una optimización completa con un espacio reducido centrado en los mejores valores de la optimización ligera, hemos obtenido un resultado de:\n",
    " 0.798114 |             14 |           0.206931 |       0.0162622 |       230 |           0 |                  94 |          0.0166739 |           0.084112  |           39 |   0.106233  |      37.0253 | -0.787348 |\n",
    "\n",
    "Y si hacemos las pruebas sobre el conjunto de test, el resultado obtenido es:\n",
    "-AUC en test: 0.790970334383\n",
    "\n",
    "Tras una primera prueba con variables seleccionadas, threshold >0, obtenemos un resultado de:\n",
    "0.799599 |             14 |           0.25449  |       0.0150386 |       230 |           0 |                  88 |          0.0153521 |           0.0475698 |           37 |   0.0846783 |      57.3954 | -0.787126 |\n",
    "Y si realizamos las pruebas sobre el conjunto de test:\n",
    "-AUC en test: 0.7901388712105\n",
    " Tras añadir el csv v5 hemos obtenido un resultado de la optimización completa con todo el csv de:\n",
    " |    |   bagging_fraction |   bagging_freq |   feature_fraction |   learning_rate |   max_bin |   max_depth |   min_child_samples |   min_child_weight |   min_gain_to_split |   num_leaves |   reg_alpha |   reg_lambda |      loss |\r\n",
    "|----+--------------------+----------------+--------------------+-----------------+-----------+-------------+---------------------+--------------------+---------------------+--------------+-------------+--------------+-----------|\r\n",
    "| 77 |           0.75008  |             14 |           0.155811 |       0.0171274 |       255 |           0 |                  93 |          0.0159414 |           0.073965  |           44 |   0.0658362 |      39.2701 | -0.788679\n",
    "\n",
    "Y si realizamos estas pruebas sobre el test:\n",
    "AUC en test: 0.792857925817487\n",
    "\n",
    "\n",
    "Tras añadir el csv v6 y realizar una prueba ligera sobre el csv completo obtenemos:\n",
    "|    |   bagging_fraction |   bagging_freq |   feature_fraction |   learning_rate |   max_bin |   max_depth |   min_child_samples |   min_child_weight |   min_gain_to_split |   num_leaves |   reg_alpha |   reg_lambda |      loss |\r\n",
    "|----+--------------------+----------------+--------------------+-----------------+-----------+-------------+---------------------+--------------------+---------------------+--------------+-------------+--------------+-----------|\r\n",
    "| 31 |           0.997003 |             15 |           0.177032 |       0.0298061 |       270 |           0 |                 100 |         0.0106796  |           0.0802473 |           43 |   0.115772  |      41.9076 | -0.789829\n",
    "\n",
    "Con un espacio más centrado en esta anterior optimización y más evaluaciones obtenemos:\n",
    "|    |   bagging_fraction |   bagging_freq |   feature_fraction |   learning_rate |   max_bin |   max_depth |   min_child_samples |   min_child_weight |   min_gain_to_split |   num_leaves |   reg_alpha |   reg_lambda |      loss |\r\n",
    "|----+--------------------+----------------+--------------------+-----------------+-----------+-------------+---------------------+--------------------+---------------------+--------------+-------------+--------------+-----------|\r\n",
    "| 69 |           0.962185 |             18 |           0.132123 |       0.016134  |       230 |           0 |                  93 |          0.0127495 |           0.0578221 |           42 |   0.0866981 |      39.8677 | -0.793874\n",
    "\n",
    "Y sobre el test AUC en test: 0.796208526365374\n",
    "\n",
    "Tras añadir finalmente todos los cambios realizado en todas las tablas:\n",
    "Optimización ligera:\n",
    "|    |   bagging_fraction |   bagging_freq |   feature_fraction |   learning_rate |   max_bin |   max_depth |   min_child_samples |   min_child_weight |   min_gain_to_split |   num_leaves |   reg_alpha |   reg_lambda |      loss |\r\n",
    "|----+--------------------+----------------+--------------------+-----------------+-----------+-------------+---------------------+--------------------+---------------------+--------------+-------------+--------------+-----------|\r\n",
    "| 24 |           0.99904  |             13 |           0.248436 |       0.0297273 |       300 |           0 |                  95 |          0.0135347 |           0.0569547 |           44 |   0.186123  |     10.5439  | -0.791107\n",
    "\n",
    "Optimización Completa:\n",
    "|    |   bagging_fraction |   bagging_freq |   feature_fraction |   learning_rate |   max_bin |   max_depth |   min_child_samples |   min_child_weight |   min_gain_to_split |   num_leaves |   reg_alpha |   reg_lambda |      loss |\r\n",
    "|----+--------------------+----------------+--------------------+-----------------+-----------+-------------+---------------------+--------------------+---------------------+--------------+-------------+--------------+-----------|\r\n",
    "| 69 |           0.891186 |             16 |           0.106319 |       0.0173115 |       300 |           0 |                 101 |          0.0137934 |           0.0269529 |           41 |   0.0906638 |      31.1379 | -0.794675\n",
    "\n",
    "Y sobre el test:\n",
    "AUC en test: 0.798\n",
    "\n",
    "Si para este mismo csv V7 realizamos una permutation importance con threshold > 0 obtenemos:\n",
    "\n",
    "| 50 |           0.940211 |             18 |           0.114142 |       0.017795  |       280 |           0 |                  97 |          0.0131429 |           0.0344192 |           35 |   0.101093  |      23.3484 | -0.794194 |\n",
    "\n",
    "Y si probamos sobre el test:\n",
    "\n",
    "AUC en test: 0.7955353053650761\n",
    "\n",
    "\n",
    "Realizando pruebas con un rango de espacio más amplio sobre todo de cara a ver como varía la profundidad obtenemos:\n",
    "| 97 |           0.985295 |             12 |           0.172079 |       0.0224613 |       275 |           3 |                 240 |          0.011828  |           0.0916892 |           2300 |          131 |   0.0738748 |      77.5059 | -0.793815 |\n",
    "\n",
    "AUC en test: 0.795\n",
    "Realizando pruebas con un espacio más acotado hemos obtenido:\n",
    "-+-------------+--------------+-----------+\r\n",
    "|    |   bagging_fraction |   bagging_freq |   feature_fraction |   learning_rate |   max_bin |   max_depth |   min_child_samples |   min_child_weight |   min_gain_to_split |   n_estimators |   num_leaves |   reg_alpha |   reg_lambda |      loss |\r\n",
    "|----+--------------------+----------------+--------------------+-----------------+-----------+-------------+---------------------+--------------------+---------------------+----------------+--------------+-------------+--------------+-----------|\r\n",
    "| 38 |           0.960846 |             15 |           0.131173 |       0.0175334 |       295 |           4 |                 220 |          0.010008  |           0.0246018 |           2200 |          247 |   0.103971  |      46.9502 | -0.794535\n",
    "Pero tras realizar pruebas sobre el test:\n",
    "AUC en test: 0.7957303444675893 vemos que la mejora no es tan grande como cuando no teniamos límite de profundidad, indicando que este número excesivo de hojas posiblemente esté casuando over fitting\n",
    "\n",
    "\n",
    "Mejor resultado obtenido en el csv reducido tras realizar varias pruebas. No se acerca al del csv completo. Toca ver como rinde\n",
    "realizando una submisión en Kaggle. En caso negativo probar CNN para selección de variables o pasar directamente a stacking para acabar proyecto.\n",
    " AUC en test: 0.797860519764853 |\n",
    " | | | | | | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg-py3.12",
   "language": "python",
   "name": "tfg-py3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
