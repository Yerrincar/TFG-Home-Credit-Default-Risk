{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "Directorio actual: c:\\Users\\Yeray\\Desktop\\DATA_SCIENCE_ML\\Home-Credit-TFG\\JUPYTER_NOTEBOOKS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nbureau = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/bureau.csv')\\nbureau_balance = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/bureau_balance.csv')\\ncredit_card_balance = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/credit_card_balance.csv')\\ninstallments_payments = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/installments_payments.csv')\\npos_cash_balance = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/POS_CASH_balance.csv')\\nprevious_application = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/previous_application.csv')\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Celda para librerías\n",
    "import sklearn as sk\n",
    "\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# Modelos base de Machine Learning que vamos a utilizar para hacer pruebas iniciales\n",
    "#La métrica usada para medir los modelos será el AUC-ROC\n",
    "# LightGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "#KGBoost\n",
    "from xgboost import XGBClassifier\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(np.__version__)\n",
    "from ydata_profiling import ProfileReport\n",
    "import os\n",
    "print(\"Directorio actual:\", os.getcwd())\n",
    "\n",
    "\n",
    "\n",
    "# Have all columns appear when dataframes are displayed.\n",
    "pd.set_option('display.max_columns', None) \n",
    "# Have 100 rows appear when a dataframe is displayed\n",
    "pd.set_option('display.max_rows', 500)\n",
    "# Display dimensions whenever a dataframe is printed out.\n",
    "pd.set_option('display.show_dimensions', True)\n",
    "\n",
    "\n",
    "#Importando los datos\n",
    "\n",
    "app_train = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/application_train.csv')\n",
    "app_test = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/application_test.csv')\n",
    "\"\"\"\n",
    "bureau = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/bureau.csv')\n",
    "bureau_balance = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/bureau_balance.csv')\n",
    "credit_card_balance = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/credit_card_balance.csv')\n",
    "installments_payments = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/installments_payments.csv')\n",
    "pos_cash_balance = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/POS_CASH_balance.csv')\n",
    "previous_application = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/previous_application.csv')\n",
    "\"\"\"\n",
    "#info = app_train.info()\n",
    "#print(info)\n",
    "#describe = app_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "\n",
    "\n",
    "app_train.drop(columns=['SK_ID_CURR'], inplace=True)\n",
    "#Debido a un fallo que detecta que todas los valores de la columna DAYS_BIRTH \n",
    "# son nan cuando no es así, vamos a sustituir la columna por una nueva llamada AGE_INT\n",
    "app_train['AGE_INT'] = app_train['DAYS_BIRTH']/-365\n",
    "app_train.drop(columns=['DAYS_BIRTH'], inplace=True)\n",
    "\n",
    "\n",
    "#Vamos a convertir la variable CNT_CHILDREN en una variable categórica que indique 1 si tiene hijos y 0 si no.\n",
    "app_train['HAS_CHILDREN'] = np.where(app_train['CNT_CHILDREN'] > 0, 1, 0)\n",
    "app_train.drop(columns=['CNT_CHILDREN'], inplace=True)\n",
    "#Vamos a crear la variable DAYS_EMPLOYED en una variable categórica que indique 1 si tiene empleo y 0 si no.\n",
    "app_train['HAS_EMPLOYMENT'] = app_train['DAYS_EMPLOYED'].map(lambda x: 1 if x < 0 else 0)\n",
    "\n",
    "#Voy a crear RATIOS para algunas de las variables más importantes (en mi opinión) y que además tienen valores atípicos.\n",
    "app_train['CREDIT_AMT_INCOME_RATIO'] = app_train['AMT_CREDIT']/app_train['AMT_INCOME_TOTAL']\n",
    "app_train['ANNUITY_AMT_INCOME_RATIO'] = app_train['AMT_ANNUITY']/app_train['AMT_INCOME_TOTAL']\n",
    "app_train['CREDIT_ANNUITY_RATIO'] = app_train['AMT_CREDIT']/app_train['AMT_ANNUITY']\n",
    "app_train['CREDIT_GOODS_PRICE_RATIO'] = app_train['AMT_CREDIT']/app_train['AMT_GOODS_PRICE']\n",
    "app_train['GOODS_INCOME_RATIO'] = app_train['AMT_GOODS_PRICE']/app_train['AMT_INCOME_TOTAL']\n",
    "\n",
    "\n",
    "#EXT_SOURCE son variables representan fuentes externas de información sobre el cliente, y suelen estar relacionadas con puntuaciones de riesgo crediticio generadas por \n",
    "# instituciones externas al prestamista, como burós de crédito u otras entidades que evalúan el perfil financiero de los clientes.\n",
    "#Vamos a crear variables relacionadas con EXT_SOURCE.\n",
    "\n",
    "app_train['EXT_SOURCE_SUM'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].sum(axis=1)\n",
    "app_train['EXT_SOURCE_MEAN'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "app_train['EXT_SOURCE_MEDIAN'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].median(axis=1)\n",
    "app_train['EXT_SOURCE_MAX'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].max(axis=1)\n",
    "app_train['EXT_SOURCE_MIN'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].min(axis=1)\n",
    "\n",
    "app_train['EXT_SOURCE_WEIGHTED_SUM'] = app_train['EXT_SOURCE_1']*2 + app_train['EXT_SOURCE_2']*3 + app_train['EXT_SOURCE_3']*4\n",
    "app_train['EXT_SOURCE_WEIGHTED_MEAN'] = (app_train['EXT_SOURCE_1']*2 + app_train['EXT_SOURCE_2']*3 + app_train['EXT_SOURCE_3']*4)/3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en app_train después de la imputación:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento previo a la creación de los modelos\n",
    "# Imputación de valores nulos. Vamos a probar de momento a completar variables numéricos con la media\n",
    "# y las categóricas con la moda.\n",
    "# Como sabemos que DAYS_EMPLOYEED tiene un valor erroneo, vamos a reemplazarlo por NaN\n",
    "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "# Algunas variables categóricas tienen valores como XNA, vamos a reemplazarlos por NaN\n",
    "def reemplazar_xna_por_nan(app_train):\n",
    "    variables_categoricas = app_train.select_dtypes(include=['object']).columns\n",
    "    for col in variables_categoricas:\n",
    "        app_train[col] = app_train[col].replace('XNA', np.nan)\n",
    "    return app_train\n",
    "app_train_noxna = reemplazar_xna_por_nan(app_train)\n",
    "\n",
    "app_train = reemplazar_xna_por_nan(app_train)\n",
    "def imputar_valores_nulos(app_train):\n",
    "    variables_continuas = app_train.select_dtypes(include=['int64', 'float64']).columns \n",
    "    for col in variables_continuas:\n",
    "        app_train[col] = app_train[col].fillna(app_train[col].mean())\n",
    "    variables_categoricas = app_train.select_dtypes(include=['object']).columns\n",
    "    for col in variables_categoricas:\n",
    "        app_train[col] = app_train[col].fillna(app_train[col].mode()[0])\n",
    "\n",
    "    return app_train\n",
    "\n",
    "app_train_nonan = imputar_valores_nulos(app_train_noxna)\n",
    "#Comprobamos que no haya valores nulos\n",
    "#print(app_train['DAYS_BIRTH'].unique())\n",
    "#print(app_train['DAYS_BIRTH'].isnull().sum())\n",
    "#print(app_train['DAYS_BIRTH'].dtype)\n",
    "\n",
    "# Verificar si hay valores nulos en app_train después de la imputación\n",
    "print(\"Valores nulos en app_train después de la imputación:\")\n",
    "print(app_train_nonan.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en app_train después de la imputación:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Tratamiento de variables categóricas\n",
    "# Vamos a probar con Label Encoding para las variables categoricas binarias y con \n",
    "# One Hot Encoding para el resto.\n",
    "\n",
    "variables_categoricas = app_train.select_dtypes(include=['object']).columns\n",
    "variables_categoricas_binarias = [col for col in variables_categoricas if app_train[col].nunique() == 2]\n",
    "variables_categoricas_no_binarias = [col for col in variables_categoricas if app_train[col].nunique() > 2]\n",
    "def label_encoding_binarias(app_train):\n",
    "    Encoder = LabelEncoder()\n",
    "    for col in variables_categoricas_binarias:\n",
    "        app_train[col] = Encoder.fit_transform(app_train[col])\n",
    "    return app_train\n",
    "\n",
    "app_train_nonan_le = label_encoding_binarias(app_train_nonan)\n",
    "\n",
    "\n",
    "def one_hot_encoding_no_binarrias(app_train):\n",
    "    for col in variables_categoricas_no_binarias:\n",
    "        app_train = pd.get_dummies(app_train, columns=[col])\n",
    "    return app_train\n",
    "\n",
    "app_train_nonan_le_oh = one_hot_encoding_no_binarrias(app_train_nonan_le)\n",
    "print(\"Valores nulos en app_train después de la imputación:\")\n",
    "print(app_train_nonan_le_oh.isnull().sum().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en app_train después de la imputación:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Normalización de las variables\n",
    "# Como gracias a la exploración de datos hemos visto que hay variables que ya han sido normalizadas \n",
    "# previamente, nos vamos a centrar en aquellas que necesitan ser normalizadas.\n",
    "variables_continuas = app_train_nonan_le_oh.select_dtypes(include=['int64', 'float64']).columns\n",
    "variables_normalizadas = [col for col in variables_continuas if\n",
    "                          app_train[col].min() >= 0 and app_train[col].max() <= 1]\n",
    "variables_NO_normalizadas = [col for col in variables_continuas if col not in variables_normalizadas]\n",
    "\n",
    "# Vamos a probar con Log Transformation por el momento debido a que tienen bastantes valores atípicos\n",
    "# log1p transforma los valores negativos en nan por lo que vamos a usar RobustScaler para evitarlo\n",
    "scaler = RobustScaler()\n",
    "#Voy a probar también con MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler2 = MinMaxScaler()\n",
    "def normalización(app_train):\n",
    "    for col in variables_NO_normalizadas:\n",
    "        app_train[col] = scaler2.fit_transform(app_train[[col]])\n",
    "    return app_train\n",
    "\n",
    "app_train_nonan_le_oh_norm = normalización(app_train_nonan_le_oh)\n",
    "print(\"Valores nulos en app_train después de la imputación:\")\n",
    "print(app_train_nonan_le_oh_norm.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tras este preprocesamiento inicial vamos a probar diferentes algoritmos. La métrica usada será AUC_ROC.\n",
    "#Vamos a probar con los siguientes algoritmos:LightGBM, XGBoost, KNN, Neural Network y Random Forest\n",
    "x = app_train_nonan_le_oh_norm.drop(columns=['TARGET'])\n",
    "y = app_train_nonan_le_oh_norm['TARGET']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados de los modelos conforme he ido haceindo cambios\n",
    "#LightGBM\n",
    "-LightGBM AUC: 0.7536 (Log Transformation)\n",
    "-LightGBM AUC: 0.7576 (Robust Scaler)\n",
    "-LightGBM AUC: 0.7577 (MinMax)\n",
    "-LightGBM AUC: 0.7577 (Se me había pasado reemplazar algunos valores XNA en variables categóricas)\n",
    "-LightGBM AUC: 0.7578 (Variable HAS_CHILDREN y HAS_EMPLOYMENT añadida. Variable CNT_CHILDREN eliminada)\n",
    "-LightGBM AUC: 0.7664 (He creado 5 nuevas, RATIOS de algunas variables más \"importantes\")\n",
    "-LightGBM AUC: 0.7654 (Tras añadir variables relacionadas con EXT_SOURCE)\n",
    "\n",
    "#XGBoost\n",
    "-XGBoost AUC: 0.7491\n",
    "-XGBoost AUC: 0.7481\n",
    "-XGBoost AUC: 0.7481\n",
    "-XGBoost AUC: 0.7488\n",
    "-XGBoost AUC: 0.7491\n",
    "-XGBoost AUC: 0.7599\n",
    "-XGBoost AUC: 0.7588\n",
    "\n",
    "#Random Forest\n",
    "-Random Forest AUC: 0.7068\n",
    "-Random Forest AUC: 0.7136\n",
    "-Random Forest AUC: 0.7138\n",
    "-Random Forest AUC: 0.7154\n",
    "-Random Forest AUC: 0.7097\n",
    "-Random Forest AUC: 0.7152\n",
    "-Random Forest AUC: 0.7236\n",
    "\n",
    "#Extra Trees\n",
    "-Extra Trees AUC: 0.6981\n",
    "-Extra Trees AUC: 0.7052\n",
    "-Extra Trees AUC: 0.7048\n",
    "-Extra Trees AUC: 0.7038\n",
    "-Extra Trees AUC: 0.7053\n",
    "-Extra Trees AUC: 0.7077\n",
    "-Extra Trees AUC: 0.7262\n",
    "\n",
    "#Logistic Regresion\n",
    "-Logistic Regression AUC: 0.7417\n",
    "-Logistic Regression AUC: 0.7478\n",
    "-Logistic Regression AUC: 0.7457\n",
    "-Logistic Regression AUC: 0.7452\n",
    "-Logistic Regression AUC: 0.7459\n",
    "-Logistic Regression AUC: 0.7480\n",
    "-Logistic Regression AUC: 0.7497\n",
    "\n",
    "#Neural Network\n",
    "-Neural Network AUC: 0.6861\n",
    "-Neural Network AUC: 0.7437 (Como la primera vez tardó demasiado lo descarté, pero tras el último cambio, Variable HAS_CHILDREN y HAS_EMPLOYMENT añadida, he probado a ejecutarla e interrumpirla pronto y me ha dado un buen resultado)\n",
    "\n",
    "#Modelos descartados\n",
    "\n",
    "#KNN (He decidido no seguir haciendo pruebas con KNN. En comparación con los otros modelos los resultados son más bajos y cada vez que refino el proceso empeora)\n",
    "-KNN AUC: 0.5482\n",
    "-KNN AUC: 0.5575\n",
    "-KNN AUC: 0.5497\n",
    "-KNN AUC: 0.5471\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19876, number of negative: 226132\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14516\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 240\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080794 -> initscore=-2.431606\n",
      "[LightGBM] [Info] Start training from score -2.431606\n",
      "LightGBM AUC: 0.7654\n"
     ]
    }
   ],
   "source": [
    "#LightGBM\n",
    "\n",
    "\n",
    "# Remove special characters from column names\n",
    "X_train.columns = X_train.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "X_test.columns = X_test.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "\n",
    "lgb_model = LGBMClassifier(random_state=42)\n",
    "lgb_model.fit(X_train, Y_train)\n",
    "lgb_predictions = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#Evaluación\n",
    "lgb_auc = roc_auc_score(Y_test, lgb_predictions)\n",
    "print(f\"LightGBM AUC: {lgb_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUC: 0.7588\n"
     ]
    }
   ],
   "source": [
    "#XGBoost\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, Y_train)\n",
    "xgb_predictions = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#Evaluación\n",
    "xgb_auc = roc_auc_score(Y_test, xgb_predictions)\n",
    "print(f\"XGBoost AUC: {xgb_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network AUC: 0.7437\n"
     ]
    }
   ],
   "source": [
    "#Neural Network\n",
    "nn_model = MLPClassifier(random_state=42)\n",
    "nn_model.fit(X_train, Y_train)\n",
    "nn_predictions = nn_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#Evaluación\n",
    "nn_auc = roc_auc_score(Y_test, nn_predictions)\n",
    "print(f\"Neural Network AUC: {nn_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest AUC: 0.7236\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, Y_train)\n",
    "rf_predictions = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#Evaluación\n",
    "rf_auc = roc_auc_score(Y_test, rf_predictions)\n",
    "print(f\"Random Forest AUC: {rf_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees AUC: 0.7262\n"
     ]
    }
   ],
   "source": [
    "#Extra Trees\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "et_model = ExtraTreesClassifier(random_state=42)\n",
    "et_model.fit(X_train, Y_train)\n",
    "et_predictions = et_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#Evaluación\n",
    "et_auc = roc_auc_score(Y_test, et_predictions) \n",
    "print(f\"Extra Trees AUC: {et_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC: 0.7497\n"
     ]
    }
   ],
   "source": [
    "#Regresión Logística\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train, Y_train)\n",
    "lr_predictions = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#Evaluación\n",
    "lr_auc = roc_auc_score(Y_test, lr_predictions)\n",
    "print(f\"Logistic Regression AUC: {lr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
