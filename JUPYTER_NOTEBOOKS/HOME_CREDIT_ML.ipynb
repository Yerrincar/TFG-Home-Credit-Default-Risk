{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "Directorio actual: c:\\Users\\Yeray\\Desktop\\DATA_SCIENCE_ML\\Home-Credit-TFG\\JUPYTER_NOTEBOOKS\n"
     ]
    }
   ],
   "source": [
    "#Celda para librerías\n",
    "import sklearn as sk\n",
    "\n",
    "\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# LightGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "#KGBoost\n",
    "from xgboost import XGBClassifier\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(np.__version__)\n",
    "from ydata_profiling import ProfileReport\n",
    "import os\n",
    "print(\"Directorio actual:\", os.getcwd())\n",
    "\n",
    "\n",
    "\n",
    "# Have all columns appear when dataframes are displayed.\n",
    "pd.set_option('display.max_columns', None) \n",
    "# Have 100 rows appear when a dataframe is displayed\n",
    "pd.set_option('display.max_rows', 500)\n",
    "# Display dimensions whenever a dataframe is printed out.\n",
    "pd.set_option('display.show_dimensions', True)\n",
    "\n",
    "\n",
    "#Importando los datos\n",
    "\n",
    "app_train = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/application_train.csv')\n",
    "app_test = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/application_test.csv')\n",
    "\"\"\"\n",
    "bureau = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/bureau.csv')\n",
    "bureau_balance = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/bureau_balance.csv')\n",
    "credit_card_balance = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/credit_card_balance.csv')\n",
    "installments_payments = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/installments_payments.csv')\n",
    "pos_cash_balance = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/POS_CASH_balance.csv')\n",
    "previous_application = pd.read_csv(r'C:/Users/Yeray/Desktop/DATA_SCIENCE_ML/Home-Credit-TFG/DATA/home-credit-default-risk/previous_application.csv')\n",
    "\"\"\"\n",
    "app_train.drop(columns=['SK_ID_CURR'], inplace=True)\n",
    "#Debido a un fallo que detecta que todas los valores de la columna DAYS_BIRTH \n",
    "# son nan cuando no es así, vamos a sustituir la columna por una nueva llamada AGE_INT\n",
    "app_train['AGE_INT'] = app_train['DAYS_BIRTH']/-365\n",
    "app_train['HAS_CHILDREN'] = np.where(app_train['CNT_CHILDREN'] > 0, 1, 0)\n",
    "\n",
    "#info = app_train.info()\n",
    "#print(info)}\n",
    "#describe = app_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "\n",
    "\n",
    "#Vamos a convertir la variable CNT_CHILDREN en una variable categórica que indique 1 si tiene hijos y 0 si no.\n",
    "\n",
    "#Vamos a crear la variable DAYS_EMPLOYED en una variable categórica que indique 1 si tiene empleo y 0 si no.\n",
    "app_train['HAS_EMPLOYMENT'] = app_train['DAYS_EMPLOYED'].map(lambda x: 1 if x < 0 else 0)\n",
    "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "\n",
    "\n",
    "#Voy a crear RATIOS para algunas de las variables más importantes (en mi opinión y según lo investigado) \n",
    "# y que además tienen valores atípicos.\n",
    "app_train['CREDIT_AMT_INCOME_RATIO'] = app_train['AMT_CREDIT']/app_train['AMT_INCOME_TOTAL']\n",
    "app_train['CREDIT_AMT_INCOME_PORCENTAJE'] = app_train['AMT_INCOME_TOTAL']/app_train['AMT_CREDIT']\n",
    "\n",
    "app_train['ANNUITY_AMT_INCOME_RATIO'] = app_train['AMT_INCOME_TOTAL']/app_train['AMT_ANNUITY']\n",
    "app_train['ANNUITY_INCOME_PORCENTAJE'] = app_train['AMT_ANNUITY']/app_train['AMT_INCOME_TOTAL']\n",
    "\n",
    "app_train['GOODS_INCOME_RATIO'] = app_train['AMT_GOODS_PRICE']/app_train['AMT_INCOME_TOTAL']\n",
    "\n",
    "app_train['CREDIT_ANNUITY_RATIO'] = app_train['AMT_CREDIT']/app_train['AMT_ANNUITY']\n",
    "app_train['ANNUITY_CREDIT_PORCETAJE'] = app_train['AMT_ANNUITY']/app_train['AMT_CREDIT']\n",
    "app_train['CREDIT_GOODS_PRICE_RATIO'] = app_train['AMT_CREDIT']/app_train['AMT_GOODS_PRICE']\n",
    "\n",
    "app_train['ANNUITY_TO_DAYS_EMPLOYED_RATIO'] = app_train['AMT_ANNUITY']/app_train['DAYS_EMPLOYED']\n",
    "app_train['ANNUITY_TO_CHILDREN_RATIO'] = app_train['AMT_ANNUITY']/app_train['CNT_CHILDREN']\n",
    "\n",
    "app_train['INCOME_PER_CHILD'] = app_train['AMT_INCOME_TOTAL']/app_train['CNT_CHILDREN']\n",
    "app_train['CREDIT_PER_CHILD'] = app_train['AMT_CREDIT']/app_train['CNT_CHILDREN']\n",
    "\n",
    "app_train['AMT_INCOME_FAMILY_MEMBERS_RATIO'] = app_train['AMT_INCOME_TOTAL']/app_train['CNT_FAM_MEMBERS']\n",
    "app_train['CHILDREN_RATIO'] = app_train['CNT_CHILDREN']/app_train['CNT_FAM_MEMBERS']\n",
    "app_train['ADULTS_MEMBERS_DIFF'] = app_train['CNT_FAM_MEMBERS'] - app_train['CNT_CHILDREN']\n",
    "\n",
    "app_train['INCOME_PER_ADULT'] = app_train['AMT_INCOME_TOTAL']/app_train['ADULTS_MEMBERS_DIFF']\n",
    "app_train['CREDIT_PER_MEMBER'] = app_train['AMT_CREDIT']/app_train['CNT_FAM_MEMBERS']\n",
    "app_train['CREDIT_PER_ADULT'] = app_train['AMT_CREDIT']/app_train['ADULTS_MEMBERS_DIFF']\n",
    "app_train['ANNUITY_PER_ADULT'] = app_train['AMT_ANNUITY']/app_train['ADULTS_MEMBERS_DIFF']\n",
    "\n",
    "\n",
    "#app_train['OWN_CAR_AGE_RATIO'] = app_train['OWN_CAR_AGE']/app_train['AGE_INT']\n",
    "app_train['NEW_CAR_BIRTH_RATIO'] = app_train['OWN_CAR_AGE']/app_train['DAYS_BIRTH']\n",
    "app_train['NEW_CAR_EMPLOYMENT_RATIO'] = app_train['OWN_CAR_AGE']/app_train['DAYS_EMPLOYED']\n",
    "\n",
    "\n",
    "app_train['EMPLOYMENT_AGE_RATIO'] = app_train['DAYS_EMPLOYED']/-app_train['AGE_INT']\n",
    "#app_train['EMPLOYMENT_BIRTH_RATIO'] = app_train['DAYS_EMPLOYED']/app_train['DAYS_BIRTH']\n",
    "app_train['EMPLOYMENT_PHONE_CHANGE_RATIO'] = app_train['DAYS_LAST_PHONE_CHANGE']/app_train['DAYS_EMPLOYED']\n",
    "\n",
    "app_train['NEW_PHONE_CHANGE_BIRTH_RATIO'] = app_train['DAYS_LAST_PHONE_CHANGE']/app_train['DAYS_BIRTH']\n",
    "app_train['NEW_PHONE_CHANGE_EMPLOYMENT_RATIO'] = app_train['DAYS_LAST_PHONE_CHANGE']/app_train['DAYS_EMPLOYED']\n",
    "app_train['DAYS_ID_PUBLISH_BIRTH_RAIO'] = app_train['DAYS_ID_PUBLISH']/app_train['DAYS_BIRTH']\n",
    "\n",
    "app_train['REGISTRATION_TO_NEW_PHONE_CHANGE_RATIO'] = app_train['DAYS_LAST_PHONE_CHANGE']/app_train['DAYS_REGISTRATION']\n",
    "#app_train['REGISTRATION_TO_ID_PUBLISH_RATIO'] = app_train['DAYS_ID_PUBLISH'] - app_train['DAYS_REGISTRATION']\n",
    "#app_train['REGISTRATION_TO_BIRTH_RATIO'] = app_train['DAYS_BIRTH'] - app_train['DAYS_REGISTRATION']\n",
    "\n",
    "#EXT_SOURCE son variables representan fuentes externas de información sobre el cliente, y suelen estar relacionadas con puntuaciones de riesgo crediticio generadas por \n",
    "# instituciones externas al prestamista, como burós de crédito u otras entidades que evalúan el perfil financiero de los clientes.\n",
    "#Vamos a crear variables relacionadas con EXT_SOURCE.\n",
    "\n",
    "app_train['EXT_SOURCE_SUM'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].sum(axis=1)\n",
    "app_train['EXT_SOURCE_MEAN'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "app_train['EXT_SOURCE_MEDIAN'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].median(axis=1)\n",
    "app_train['EXT_SOURCE_MAX'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].max(axis=1)\n",
    "app_train['EXT_SOURCE_MIN'] = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].min(axis=1)\n",
    "\n",
    "app_train['EXT_SOURCE_WEIGHTED_SUM'] = app_train['EXT_SOURCE_1']*3 + app_train['EXT_SOURCE_2']*1 + app_train['EXT_SOURCE_3']*5\n",
    "app_train['EXT_SOURCE_WEIGHTED_MEAN'] = (app_train['EXT_SOURCE_1']*3 + app_train['EXT_SOURCE_2']*1 + app_train['EXT_SOURCE_3']*5)/3\n",
    "app_train['EXT_SOURCE_PROD'] = app_train['EXT_SOURCE_1']*app_train['EXT_SOURCE_2']*app_train['EXT_SOURCE_3']\n",
    "\n",
    "app_train['RATIO_EXT_SOURCE_3_TO_REGION_POPULATION_RELATIVE'] = app_train['EXT_SOURCE_3'] / app_train['REGION_POPULATION_RELATIVE']\n",
    "\n",
    "\"\"\"\n",
    "app_train['AGE_CREDIT_RATIO'] = app_train['AGE_INT']/app_train['AMT_CREDIT']\n",
    "app_train['AGE_ANNUITY_RATIO'] = app_train['AGE_INT']/app_train['AMT_ANNUITY']\n",
    "app_train['AGE_GOODS_PRICE_RATIO'] = app_train['AGE_INT']/app_train['AMT_GOODS_PRICE']\n",
    "app_train['AGE_INCOME_RATIO'] = app_train['AGE_INT']/app_train['AMT_INCOME_TOTAL']\n",
    "\"\"\"\n",
    "app_train['AGE_CREDIT_RATIO'] = app_train['AMT_CREDIT']/app_train['DAYS_BIRTH']\n",
    "app_train['AGE_ANNUITY_RATIO'] = app_train['AMT_ANNUITY']/app_train['DAYS_BIRTH']\n",
    "app_train['AGE_GOODS_PRICE_RATIO'] = app_train['AMT_GOODS_PRICE']/app_train['DAYS_BIRTH']\n",
    "app_train['AGE_INCOME_RATIO'] = app_train['AMT_INCOME_TOTAL']/app_train['DAYS_BIRTH']\n",
    "\n",
    "\n",
    "\n",
    "#Variables a partir de sumas y diferencias\n",
    "app_train['AMT_INCOME_TOTAL_ANNUITY_SUM'] = app_train['AMT_INCOME_TOTAL'] + app_train['AMT_ANNUITY']\n",
    "app_train['AMT_GOODS_TO_INCOME_ANUITY_SUM_RATIO'] = app_train['AMT_GOODS_PRICE']/(app_train['AMT_INCOME_TOTAL_ANNUITY_SUM'] )\n",
    "app_train['CREDIT_BUREAU_TOTAL'] = app_train[['AMT_REQ_CREDIT_BUREAU_DAY', \n",
    "                                             'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    "                                             'AMT_REQ_CREDIT_BUREAU_MON',\n",
    "                                             'AMT_REQ_CREDIT_BUREAU_QRT',\n",
    "                                             'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "                                             'AMT_REQ_CREDIT_BUREAU_YEAR']].sum(axis=1)\n",
    "\n",
    "app_train['CREDIT_GOODS_DIFF'] = app_train['AMT_GOODS_PRICE'] - app_train['AMT_CREDIT']\n",
    "app_train['GOODS_ANNUITY_DIFF'] = app_train['AMT_ANNUITY'] - app_train['AMT_GOODS_PRICE']\n",
    "app_train['AMT_INCOME_TOTAL_ANNUITY_DIFF'] = app_train['AMT_INCOME_TOTAL'] - app_train['AMT_ANNUITY']\n",
    "\n",
    "#app_train['SOCIAL_OBSERVATION_TOTAL'] = app_train['OBS_30_CNT_SOCIAL_CIRCLE'] + app_train['OBS_60_CNT_SOCIAL_CIRCLE']\n",
    "#app_train['SOCIAL_DEF_TOTAL'] = app_train['DEF_30_CNT_SOCIAL_CIRCLE'] + app_train['DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "\n",
    "app_train['DIFF_OBS_30_60'] = app_train['OBS_30_CNT_SOCIAL_CIRCLE'] - app_train['OBS_60_CNT_SOCIAL_CIRCLE']\n",
    "app_train['DIFF_DEF_30_60'] = app_train['DEF_30_CNT_SOCIAL_CIRCLE'] - app_train['DEF_60_CNT_SOCIAL_CIRCLE'] \n",
    "\n",
    "app_train['LONG_EMPLOYMENT'] = np.where(app_train['DAYS_EMPLOYED'] < -2000, 1, 0)\n",
    "app_train['RETIRED'] = np.where(app_train['DAYS_BIRTH'] < -14000, 1, 0)\n",
    "\n",
    "#OTROS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    307511.000000\n",
      "mean          2.509507\n",
      "std           0.874544\n",
      "min           1.000000\n",
      "25%           2.000000\n",
      "50%           2.000000\n",
      "75%           3.000000\n",
      "max           5.000000\n",
      "Name: EDUCATION_LEVEL, Length: 8, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nincome_type_mapping = {\\n    'Unemployed': 0,\\n    'Student': 1,\\n    'Pensioner': 2,\\n    'Maternity leave': 3,\\n    'Working': 4,\\n    'State servant': 5,\\n    'Commercial associate': 6,\\n    'Businessman': 7,    \\n}\\n\\nfamily_mapping = {\\n    'Single / not married': 0,\\n    'Separated': 1,\\n    'Widow': 2,\\n    'Married': 3\\n}\\n\\nhousing_mapping = {\\n    'With parents': 0,\\n    'Municipal apartment': 1,\\n    'Rented apartment': 2,\\n    'House / apartment': 3\\n}\\n\\n\\n\\napp_train['INCOME_TYPE'] = app_train['NAME_INCOME_TYPE'].map(income_type_mapping)\\napp_train['FAMILY_STATUS'] = app_train['NAME_FAMILY_STATUS'].map(family_mapping)\\napp_train['HOUSING_TYPE'] = app_train['NAME_HOUSING_TYPE'].map(housing_mapping)\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "education_mapping = {\n",
    "    'No formal education': 0,\n",
    "    'Lower secondary': 1,\n",
    "    'Secondary / secondary special': 2,\n",
    "    'Incomplete higher': 3,\n",
    "    'Higher education': 4,\n",
    "    'Academic degree': 5\n",
    "}\n",
    "\n",
    "app_train['EDUCATION_LEVEL'] = app_train['NAME_EDUCATION_TYPE'].map(education_mapping)\n",
    "print(app_train['EDUCATION_LEVEL'].describe())\n",
    "\"\"\"\n",
    "income_type_mapping = {\n",
    "    'Unemployed': 0,\n",
    "    'Student': 1,\n",
    "    'Pensioner': 2,\n",
    "    'Maternity leave': 3,\n",
    "    'Working': 4,\n",
    "    'State servant': 5,\n",
    "    'Commercial associate': 6,\n",
    "    'Businessman': 7,    \n",
    "}\n",
    "\n",
    "family_mapping = {\n",
    "    'Single / not married': 0,\n",
    "    'Separated': 1,\n",
    "    'Widow': 2,\n",
    "    'Married': 3\n",
    "}\n",
    "\n",
    "housing_mapping = {\n",
    "    'With parents': 0,\n",
    "    'Municipal apartment': 1,\n",
    "    'Rented apartment': 2,\n",
    "    'House / apartment': 3\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "app_train['INCOME_TYPE'] = app_train['NAME_INCOME_TYPE'].map(income_type_mapping)\n",
    "app_train['FAMILY_STATUS'] = app_train['NAME_FAMILY_STATUS'].map(family_mapping)\n",
    "app_train['HOUSING_TYPE'] = app_train['NAME_HOUSING_TYPE'].map(housing_mapping)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en app_train después de la imputación:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento previo a la creación de los modelos\n",
    "# Imputación de valores nulos. Vamos a probar de momento a completar variables numéricos con la media\n",
    "# y las categóricas con la moda.\n",
    "# Como sabemos que DAYS_EMPLOYEED tiene un valor erroneo, vamos a reemplazarlo por NaN\n",
    "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "# Reemplazar valores infinitos por NaN\n",
    "app_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# Algunas variables categóricas tienen valores como XNA, vamos a reemplazarlos por NaN\n",
    "def reemplazar_xna_por_nan(app_train):\n",
    "    variables_categoricas = app_train.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in variables_categoricas:\n",
    "        app_train[col] = app_train[col].replace('XNA', np.nan)\n",
    "    return app_train\n",
    "app_train_noxna = reemplazar_xna_por_nan(app_train)\n",
    "\n",
    "app_train = reemplazar_xna_por_nan(app_train)\n",
    "def imputar_valores_nulos(app_train):\n",
    "    variables_continuas = app_train.select_dtypes(include=['int64', 'float64']).columns \n",
    "    for col in variables_continuas:\n",
    "        app_train[col] = app_train[col].fillna(app_train[col].mean())\n",
    "    variables_categoricas = app_train.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in variables_categoricas:\n",
    "        app_train[col] = app_train[col].fillna(app_train[col].mode()[0])\n",
    "\n",
    "    return app_train\n",
    "\n",
    "app_train_nonan = imputar_valores_nulos(app_train_noxna)\n",
    "#Comprobamos que no haya valores nulos\n",
    "#print(app_train['DAYS_BIRTH'].unique())\n",
    "#print(app_train['DAYS_BIRTH'].isnull().sum())\n",
    "#print(app_train['DAYS_BIRTH'].dtype)\n",
    "\n",
    "# Verificar si hay valores nulos en app_train después de la imputación\n",
    "print(\"Valores nulos en app_train después de la imputación:\")\n",
    "print(app_train_nonan.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en app_train después de la imputación:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Tratamiento de variables categóricas\n",
    "# Vamos a probar con Label Encoding para las variables categoricas binarias y con \n",
    "# One Hot Encoding para el resto.\n",
    "\n",
    "variables_categoricas = app_train.select_dtypes(include=['object', 'category']).columns\n",
    "variables_categoricas_binarias = [col for col in variables_categoricas if app_train[col].nunique() == 2]\n",
    "variables_categoricas_no_binarias = [col for col in variables_categoricas if app_train[col].nunique() > 2]\n",
    "def label_encoding_binarias(app_train):\n",
    "    Encoder = LabelEncoder()\n",
    "    for col in variables_categoricas_binarias:\n",
    "        app_train[col] = Encoder.fit_transform(app_train[col])\n",
    "    return app_train\n",
    "\n",
    "app_train_nonan_le = label_encoding_binarias(app_train_nonan)\n",
    "\n",
    "\n",
    "def one_hot_encoding_no_binarrias(app_train):\n",
    "    for col in variables_categoricas_no_binarias:\n",
    "        app_train = pd.get_dummies(app_train, columns=[col])\n",
    "    return app_train\n",
    "\n",
    "app_train_nonan_le_oh = one_hot_encoding_no_binarrias(app_train_nonan_le)\n",
    "print(\"Valores nulos en app_train después de la imputación:\")\n",
    "print(app_train_nonan_le_oh.isnull().sum().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET                                                         1\n",
      "NAME_CONTRACT_TYPE                                             1\n",
      "CODE_GENDER                                                    1\n",
      "FLAG_OWN_CAR                                                   1\n",
      "FLAG_OWN_REALTY                                                1\n",
      "CNT_CHILDREN                                                  19\n",
      "AMT_INCOME_TOTAL                                     117000000.0\n",
      "AMT_CREDIT                                             4050000.0\n",
      "AMT_ANNUITY                                             258025.5\n",
      "AMT_GOODS_PRICE                                        4050000.0\n",
      "REGION_POPULATION_RELATIVE                              0.072508\n",
      "DAYS_BIRTH                                                 -7489\n",
      "DAYS_EMPLOYED                                                0.0\n",
      "DAYS_REGISTRATION                                            0.0\n",
      "DAYS_ID_PUBLISH                                                0\n",
      "OWN_CAR_AGE                                                 91.0\n",
      "FLAG_MOBIL                                                     1\n",
      "FLAG_EMP_PHONE                                                 1\n",
      "FLAG_WORK_PHONE                                                1\n",
      "FLAG_CONT_MOBILE                                               1\n",
      "FLAG_PHONE                                                     1\n",
      "FLAG_EMAIL                                                     1\n",
      "CNT_FAM_MEMBERS                                             20.0\n",
      "REGION_RATING_CLIENT                                           3\n",
      "REGION_RATING_CLIENT_W_CITY                                    3\n",
      "HOUR_APPR_PROCESS_START                                       23\n",
      "REG_REGION_NOT_LIVE_REGION                                     1\n",
      "REG_REGION_NOT_WORK_REGION                                     1\n",
      "LIVE_REGION_NOT_WORK_REGION                                    1\n",
      "REG_CITY_NOT_LIVE_CITY                                         1\n",
      "REG_CITY_NOT_WORK_CITY                                         1\n",
      "LIVE_CITY_NOT_WORK_CITY                                        1\n",
      "EXT_SOURCE_1                                            0.962693\n",
      "EXT_SOURCE_2                                               0.855\n",
      "EXT_SOURCE_3                                             0.89601\n",
      "APARTMENTS_AVG                                               1.0\n",
      "BASEMENTAREA_AVG                                             1.0\n",
      "YEARS_BEGINEXPLUATATION_AVG                                  1.0\n",
      "YEARS_BUILD_AVG                                              1.0\n",
      "COMMONAREA_AVG                                               1.0\n",
      "ELEVATORS_AVG                                                1.0\n",
      "ENTRANCES_AVG                                                1.0\n",
      "FLOORSMAX_AVG                                                1.0\n",
      "FLOORSMIN_AVG                                                1.0\n",
      "LANDAREA_AVG                                                 1.0\n",
      "LIVINGAPARTMENTS_AVG                                         1.0\n",
      "LIVINGAREA_AVG                                               1.0\n",
      "NONLIVINGAPARTMENTS_AVG                                      1.0\n",
      "NONLIVINGAREA_AVG                                            1.0\n",
      "APARTMENTS_MODE                                              1.0\n",
      "BASEMENTAREA_MODE                                            1.0\n",
      "YEARS_BEGINEXPLUATATION_MODE                                 1.0\n",
      "YEARS_BUILD_MODE                                             1.0\n",
      "COMMONAREA_MODE                                              1.0\n",
      "ELEVATORS_MODE                                               1.0\n",
      "ENTRANCES_MODE                                               1.0\n",
      "FLOORSMAX_MODE                                               1.0\n",
      "FLOORSMIN_MODE                                               1.0\n",
      "LANDAREA_MODE                                                1.0\n",
      "LIVINGAPARTMENTS_MODE                                        1.0\n",
      "LIVINGAREA_MODE                                              1.0\n",
      "NONLIVINGAPARTMENTS_MODE                                     1.0\n",
      "NONLIVINGAREA_MODE                                           1.0\n",
      "APARTMENTS_MEDI                                              1.0\n",
      "BASEMENTAREA_MEDI                                            1.0\n",
      "YEARS_BEGINEXPLUATATION_MEDI                                 1.0\n",
      "YEARS_BUILD_MEDI                                             1.0\n",
      "COMMONAREA_MEDI                                              1.0\n",
      "ELEVATORS_MEDI                                               1.0\n",
      "ENTRANCES_MEDI                                               1.0\n",
      "FLOORSMAX_MEDI                                               1.0\n",
      "FLOORSMIN_MEDI                                               1.0\n",
      "LANDAREA_MEDI                                                1.0\n",
      "LIVINGAPARTMENTS_MEDI                                        1.0\n",
      "LIVINGAREA_MEDI                                              1.0\n",
      "NONLIVINGAPARTMENTS_MEDI                                     1.0\n",
      "NONLIVINGAREA_MEDI                                           1.0\n",
      "TOTALAREA_MODE                                               1.0\n",
      "EMERGENCYSTATE_MODE                                            1\n",
      "OBS_30_CNT_SOCIAL_CIRCLE                                   348.0\n",
      "DEF_30_CNT_SOCIAL_CIRCLE                                    34.0\n",
      "OBS_60_CNT_SOCIAL_CIRCLE                                   344.0\n",
      "DEF_60_CNT_SOCIAL_CIRCLE                                    24.0\n",
      "DAYS_LAST_PHONE_CHANGE                                       0.0\n",
      "FLAG_DOCUMENT_2                                                1\n",
      "FLAG_DOCUMENT_3                                                1\n",
      "FLAG_DOCUMENT_4                                                1\n",
      "FLAG_DOCUMENT_5                                                1\n",
      "FLAG_DOCUMENT_6                                                1\n",
      "FLAG_DOCUMENT_7                                                1\n",
      "FLAG_DOCUMENT_8                                                1\n",
      "FLAG_DOCUMENT_9                                                1\n",
      "FLAG_DOCUMENT_10                                               1\n",
      "FLAG_DOCUMENT_11                                               1\n",
      "FLAG_DOCUMENT_12                                               1\n",
      "FLAG_DOCUMENT_13                                               1\n",
      "FLAG_DOCUMENT_14                                               1\n",
      "FLAG_DOCUMENT_15                                               1\n",
      "FLAG_DOCUMENT_16                                               1\n",
      "FLAG_DOCUMENT_17                                               1\n",
      "FLAG_DOCUMENT_18                                               1\n",
      "FLAG_DOCUMENT_19                                               1\n",
      "FLAG_DOCUMENT_20                                               1\n",
      "FLAG_DOCUMENT_21                                               1\n",
      "AMT_REQ_CREDIT_BUREAU_HOUR                                   4.0\n",
      "AMT_REQ_CREDIT_BUREAU_DAY                                    9.0\n",
      "AMT_REQ_CREDIT_BUREAU_WEEK                                   8.0\n",
      "AMT_REQ_CREDIT_BUREAU_MON                                   27.0\n",
      "AMT_REQ_CREDIT_BUREAU_QRT                                  261.0\n",
      "AMT_REQ_CREDIT_BUREAU_YEAR                                  25.0\n",
      "AGE_INT                                                69.120548\n",
      "HAS_CHILDREN                                                   1\n",
      "HAS_EMPLOYMENT                                                 1\n",
      "CREDIT_AMT_INCOME_RATIO                                84.736842\n",
      "CREDIT_AMT_INCOME_PORCENTAJE                          208.003328\n",
      "ANNUITY_AMT_INCOME_RATIO                             4466.586497\n",
      "ANNUITY_INCOME_PORCENTAJE                               1.875965\n",
      "GOODS_INCOME_RATIO                                     84.736842\n",
      "CREDIT_ANNUITY_RATIO                                   45.305079\n",
      "ANNUITY_CREDIT_PORCETAJE                                 0.12443\n",
      "CREDIT_GOODS_PRICE_RATIO                                     6.0\n",
      "ANNUITY_TO_DAYS_EMPLOYED_RATIO                          -0.19908\n",
      "ANNUITY_TO_CHILDREN_RATIO                               225000.0\n",
      "INCOME_PER_CHILD                                     117000000.0\n",
      "CREDIT_PER_CHILD                                       4050000.0\n",
      "AMT_INCOME_FAMILY_MEMBERS_RATIO                       39000000.0\n",
      "CHILDREN_RATIO                                              0.95\n",
      "ADULTS_MEMBERS_DIFF                                          2.0\n",
      "INCOME_PER_ADULT                                      58500000.0\n",
      "CREDIT_PER_MEMBER                                      4031032.5\n",
      "CREDIT_PER_ADULT                                       4031032.5\n",
      "ANNUITY_PER_ADULT                                       225000.0\n",
      "NEW_CAR_BIRTH_RATIO                                         -0.0\n",
      "NEW_CAR_EMPLOYMENT_RATIO                                    -0.0\n",
      "EMPLOYMENT_AGE_RATIO                                  266.016194\n",
      "EMPLOYMENT_PHONE_CHANGE_RATIO                             1784.0\n",
      "NEW_PHONE_CHANGE_BIRTH_RATIO                            0.361365\n",
      "NEW_PHONE_CHANGE_EMPLOYMENT_RATIO                         1784.0\n",
      "DAYS_ID_PUBLISH_BIRTH_RAIO                              0.469001\n",
      "REGISTRATION_TO_NEW_PHONE_CHANGE_RATIO                    3042.0\n",
      "EXT_SOURCE_SUM                                          2.560251\n",
      "EXT_SOURCE_MEAN                                         0.878903\n",
      "EXT_SOURCE_MEDIAN                                       0.885488\n",
      "EXT_SOURCE_MAX                                          0.962693\n",
      "EXT_SOURCE_MIN                                          0.878903\n",
      "EXT_SOURCE_WEIGHTED_SUM                                 7.880093\n",
      "EXT_SOURCE_WEIGHTED_MEAN                                2.626698\n",
      "EXT_SOURCE_PROD                                         0.618557\n",
      "RATIO_EXT_SOURCE_3_TO_REGION_POPULATION_RELATIVE     2440.342568\n",
      "AGE_CREDIT_RATIO                                       -1.808391\n",
      "AGE_ANNUITY_RATIO                                      -0.068777\n",
      "AGE_GOODS_PRICE_RATIO                                  -1.798417\n",
      "AGE_INCOME_RATIO                                        -1.09188\n",
      "AMT_INCOME_TOTAL_ANNUITY_SUM                         117026194.5\n",
      "AMT_GOODS_TO_INCOME_ANUITY_SUM_RATIO                   29.463796\n",
      "CREDIT_BUREAU_TOTAL                                        262.0\n",
      "CREDIT_GOODS_DIFF                                       765000.0\n",
      "GOODS_ANNUITY_DIFF                                      -31500.0\n",
      "AMT_INCOME_TOTAL_ANNUITY_DIFF                        116973805.5\n",
      "DIFF_OBS_30_60                                               4.0\n",
      "DIFF_DEF_30_60                                              10.0\n",
      "LONG_EMPLOYMENT                                                1\n",
      "RETIRED                                                        1\n",
      "EDUCATION_LEVEL                                                5\n",
      "NAME_TYPE_SUITE_Children                                    True\n",
      "NAME_TYPE_SUITE_Family                                      True\n",
      "NAME_TYPE_SUITE_Group of people                             True\n",
      "NAME_TYPE_SUITE_Other_A                                     True\n",
      "NAME_TYPE_SUITE_Other_B                                     True\n",
      "NAME_TYPE_SUITE_Spouse, partner                             True\n",
      "NAME_TYPE_SUITE_Unaccompanied                               True\n",
      "NAME_INCOME_TYPE_Businessman                                True\n",
      "NAME_INCOME_TYPE_Commercial associate                       True\n",
      "NAME_INCOME_TYPE_Maternity leave                            True\n",
      "NAME_INCOME_TYPE_Pensioner                                  True\n",
      "NAME_INCOME_TYPE_State servant                              True\n",
      "NAME_INCOME_TYPE_Student                                    True\n",
      "NAME_INCOME_TYPE_Unemployed                                 True\n",
      "NAME_INCOME_TYPE_Working                                    True\n",
      "NAME_EDUCATION_TYPE_Academic degree                         True\n",
      "NAME_EDUCATION_TYPE_Higher education                        True\n",
      "NAME_EDUCATION_TYPE_Incomplete higher                       True\n",
      "NAME_EDUCATION_TYPE_Lower secondary                         True\n",
      "NAME_EDUCATION_TYPE_Secondary / secondary special           True\n",
      "NAME_FAMILY_STATUS_Civil marriage                           True\n",
      "NAME_FAMILY_STATUS_Married                                  True\n",
      "NAME_FAMILY_STATUS_Separated                                True\n",
      "NAME_FAMILY_STATUS_Single / not married                     True\n",
      "NAME_FAMILY_STATUS_Unknown                                  True\n",
      "NAME_FAMILY_STATUS_Widow                                    True\n",
      "NAME_HOUSING_TYPE_Co-op apartment                           True\n",
      "NAME_HOUSING_TYPE_House / apartment                         True\n",
      "NAME_HOUSING_TYPE_Municipal apartment                       True\n",
      "NAME_HOUSING_TYPE_Office apartment                          True\n",
      "NAME_HOUSING_TYPE_Rented apartment                          True\n",
      "NAME_HOUSING_TYPE_With parents                              True\n",
      "OCCUPATION_TYPE_Accountants                                 True\n",
      "OCCUPATION_TYPE_Cleaning staff                              True\n",
      "OCCUPATION_TYPE_Cooking staff                               True\n",
      "OCCUPATION_TYPE_Core staff                                  True\n",
      "OCCUPATION_TYPE_Drivers                                     True\n",
      "OCCUPATION_TYPE_HR staff                                    True\n",
      "OCCUPATION_TYPE_High skill tech staff                       True\n",
      "OCCUPATION_TYPE_IT staff                                    True\n",
      "OCCUPATION_TYPE_Laborers                                    True\n",
      "OCCUPATION_TYPE_Low-skill Laborers                          True\n",
      "OCCUPATION_TYPE_Managers                                    True\n",
      "OCCUPATION_TYPE_Medicine staff                              True\n",
      "OCCUPATION_TYPE_Private service staff                       True\n",
      "OCCUPATION_TYPE_Realty agents                               True\n",
      "OCCUPATION_TYPE_Sales staff                                 True\n",
      "OCCUPATION_TYPE_Secretaries                                 True\n",
      "OCCUPATION_TYPE_Security staff                              True\n",
      "OCCUPATION_TYPE_Waiters/barmen staff                        True\n",
      "WEEKDAY_APPR_PROCESS_START_FRIDAY                           True\n",
      "WEEKDAY_APPR_PROCESS_START_MONDAY                           True\n",
      "WEEKDAY_APPR_PROCESS_START_SATURDAY                         True\n",
      "WEEKDAY_APPR_PROCESS_START_SUNDAY                           True\n",
      "WEEKDAY_APPR_PROCESS_START_THURSDAY                         True\n",
      "WEEKDAY_APPR_PROCESS_START_TUESDAY                          True\n",
      "WEEKDAY_APPR_PROCESS_START_WEDNESDAY                        True\n",
      "ORGANIZATION_TYPE_Advertising                               True\n",
      "ORGANIZATION_TYPE_Agriculture                               True\n",
      "ORGANIZATION_TYPE_Bank                                      True\n",
      "ORGANIZATION_TYPE_Business Entity Type 1                    True\n",
      "ORGANIZATION_TYPE_Business Entity Type 2                    True\n",
      "ORGANIZATION_TYPE_Business Entity Type 3                    True\n",
      "ORGANIZATION_TYPE_Cleaning                                  True\n",
      "ORGANIZATION_TYPE_Construction                              True\n",
      "ORGANIZATION_TYPE_Culture                                   True\n",
      "ORGANIZATION_TYPE_Electricity                               True\n",
      "ORGANIZATION_TYPE_Emergency                                 True\n",
      "ORGANIZATION_TYPE_Government                                True\n",
      "ORGANIZATION_TYPE_Hotel                                     True\n",
      "ORGANIZATION_TYPE_Housing                                   True\n",
      "ORGANIZATION_TYPE_Industry: type 1                          True\n",
      "ORGANIZATION_TYPE_Industry: type 10                         True\n",
      "ORGANIZATION_TYPE_Industry: type 11                         True\n",
      "ORGANIZATION_TYPE_Industry: type 12                         True\n",
      "ORGANIZATION_TYPE_Industry: type 13                         True\n",
      "ORGANIZATION_TYPE_Industry: type 2                          True\n",
      "ORGANIZATION_TYPE_Industry: type 3                          True\n",
      "ORGANIZATION_TYPE_Industry: type 4                          True\n",
      "ORGANIZATION_TYPE_Industry: type 5                          True\n",
      "ORGANIZATION_TYPE_Industry: type 6                          True\n",
      "ORGANIZATION_TYPE_Industry: type 7                          True\n",
      "ORGANIZATION_TYPE_Industry: type 8                          True\n",
      "ORGANIZATION_TYPE_Industry: type 9                          True\n",
      "ORGANIZATION_TYPE_Insurance                                 True\n",
      "ORGANIZATION_TYPE_Kindergarten                              True\n",
      "ORGANIZATION_TYPE_Legal Services                            True\n",
      "ORGANIZATION_TYPE_Medicine                                  True\n",
      "ORGANIZATION_TYPE_Military                                  True\n",
      "ORGANIZATION_TYPE_Mobile                                    True\n",
      "ORGANIZATION_TYPE_Other                                     True\n",
      "ORGANIZATION_TYPE_Police                                    True\n",
      "ORGANIZATION_TYPE_Postal                                    True\n",
      "ORGANIZATION_TYPE_Realtor                                   True\n",
      "ORGANIZATION_TYPE_Religion                                  True\n",
      "ORGANIZATION_TYPE_Restaurant                                True\n",
      "ORGANIZATION_TYPE_School                                    True\n",
      "ORGANIZATION_TYPE_Security                                  True\n",
      "ORGANIZATION_TYPE_Security Ministries                       True\n",
      "ORGANIZATION_TYPE_Self-employed                             True\n",
      "ORGANIZATION_TYPE_Services                                  True\n",
      "ORGANIZATION_TYPE_Telecom                                   True\n",
      "ORGANIZATION_TYPE_Trade: type 1                             True\n",
      "ORGANIZATION_TYPE_Trade: type 2                             True\n",
      "ORGANIZATION_TYPE_Trade: type 3                             True\n",
      "ORGANIZATION_TYPE_Trade: type 4                             True\n",
      "ORGANIZATION_TYPE_Trade: type 5                             True\n",
      "ORGANIZATION_TYPE_Trade: type 6                             True\n",
      "ORGANIZATION_TYPE_Trade: type 7                             True\n",
      "ORGANIZATION_TYPE_Transport: type 1                         True\n",
      "ORGANIZATION_TYPE_Transport: type 2                         True\n",
      "ORGANIZATION_TYPE_Transport: type 3                         True\n",
      "ORGANIZATION_TYPE_Transport: type 4                         True\n",
      "ORGANIZATION_TYPE_University                                True\n",
      "FONDKAPREMONT_MODE_not specified                            True\n",
      "FONDKAPREMONT_MODE_org spec account                         True\n",
      "FONDKAPREMONT_MODE_reg oper account                         True\n",
      "FONDKAPREMONT_MODE_reg oper spec account                    True\n",
      "HOUSETYPE_MODE_block of flats                               True\n",
      "HOUSETYPE_MODE_specific housing                             True\n",
      "HOUSETYPE_MODE_terraced house                               True\n",
      "WALLSMATERIAL_MODE_Block                                    True\n",
      "WALLSMATERIAL_MODE_Mixed                                    True\n",
      "WALLSMATERIAL_MODE_Monolithic                               True\n",
      "WALLSMATERIAL_MODE_Others                                   True\n",
      "WALLSMATERIAL_MODE_Panel                                    True\n",
      "WALLSMATERIAL_MODE_Stone, brick                             True\n",
      "WALLSMATERIAL_MODE_Wooden                                   True\n",
      "Length: 292, dtype: object\n",
      "Valores nulos en app_train después de la imputación:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Normalización de las variables\n",
    "# Como gracias a la exploración de datos hemos visto que hay variables que ya han sido normalizadas \n",
    "# previamente, nos vamos a centrar en aquellas que necesitan ser normalizadas.\n",
    "variables_continuas = app_train_nonan_le_oh.select_dtypes(include=['int64', 'float64']).columns\n",
    "variables_normalizadas = [col for col in variables_continuas if\n",
    "                          app_train[col].min() >= 0 and app_train[col].max() <= 1]\n",
    "variables_NO_normalizadas = [col for col in variables_continuas if col not in variables_normalizadas]\n",
    "\n",
    "\n",
    "# Vamos a probar con Log Transformation por el momento debido a que tienen bastantes valores atípicos\n",
    "# log1p transforma los valores negativos en nan por lo que vamos a usar RobustScaler para evitarlo\n",
    "scaler = RobustScaler()\n",
    "#Voy a probar también con MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler2 = MinMaxScaler()\n",
    "def normalización(app_train):\n",
    "    for col in variables_NO_normalizadas:\n",
    "        app_train[col] = scaler2.fit_transform(app_train[[col]])\n",
    "    return app_train\n",
    "#Valores máximos de todas las variables\n",
    "print(app_train_nonan_le_oh.max())\n",
    "app_train_nonan_le_oh_norm = normalización(app_train_nonan_le_oh)\n",
    "print(\"Valores nulos en app_train después de la imputación:\")\n",
    "print(app_train_nonan_le_oh_norm.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    307511.000000\n",
      "mean          0.081160\n",
      "std           0.065981\n",
      "min           0.000000\n",
      "25%           0.034000\n",
      "50%           0.060000\n",
      "75%           0.108000\n",
      "max           0.488000\n",
      "Name: NEIGHBORS_MEAN_TARGET, Length: 8, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Parte de Feature Engineering pero como KNN no funciona con nulos, lo colocamos aquí\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=500, metric='euclidean')\n",
    "knn.fit(app_train_nonan_le_oh_norm[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'CREDIT_ANNUITY_RATIO']])\n",
    "distances, indices = knn.kneighbors(app_train_nonan_le_oh_norm[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'CREDIT_ANNUITY_RATIO']])\n",
    "\n",
    "target_values = app_train_nonan_le_oh_norm['TARGET'].values\n",
    "neighbors_mean_target = []\n",
    "for neighbors_indices in indices:\n",
    "    neighbors_mean_target.append(target_values[neighbors_indices].mean())\n",
    "\n",
    "app_train_nonan_le_oh_norm['NEIGHBORS_MEAN_TARGET'] = neighbors_mean_target\n",
    "print(app_train_nonan_le_oh_norm['NEIGHBORS_MEAN_TARGET'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tras este preprocesamiento inicial vamos a probar diferentes algoritmos. La métrica usada será AUC_ROC.\n",
    "#Vamos a probar con los siguientes algoritmos:LightGBM, XGBoost, KNN, Neural Network y Random Forest\n",
    "x = app_train_nonan_le_oh_norm.drop(columns=['TARGET'])\n",
    "y = app_train_nonan_le_oh_norm['TARGET']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "X_train.columns = X_train.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "X_test.columns = X_test.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados de los modelos conforme he ido haciendo cambios\n",
    "Arreglar markdown que en git aparece con formato erróneo.\n",
    "#LightGBM\n",
    "-LightGBM AUC: 0.7536 (Log Transformation)\n",
    "-LightGBM AUC: 0.7576 (Robust Scaler)\n",
    "-LightGBM AUC: 0.7577 (MinMax)\n",
    "-LightGBM AUC: 0.7577 (Se me había pasado reemplazar algunos valores XNA en variables categóricas)\n",
    "-LightGBM AUC: 0.7578 (Variable HAS_CHILDREN y HAS_EMPLOYMENT añadida. Variable CNT_CHILDREN eliminada)\n",
    "-LightGBM AUC: 0.7664 (He creado 5 nuevas, RATIOS de algunas variables más \"importantes\")\n",
    "-LightGBM AUC: 0.7654 (Tras añadir variables relacionadas con EXT_SOURCE)\n",
    "-LightGBM AUC: 0.7650 (Nuevas variables en base a sumas y diferencias)\n",
    "-LightGBM AUC: 0.7647 (EDUCATION_TYPE mapped)\n",
    "\n",
    "\n",
    "\n",
    "Comienzo con la optimización de hiperparámetros\n",
    "-Mejor AUC-ROC: 0.7623980960350135\n",
    "\n",
    "He usado un space reducido de primeras solo con space = {\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 150, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -1),  # Valores entre e^(-5) y e^(-1)\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 1000, 50),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 5, 100, 5),\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.5, 1.0)\n",
    "}\n",
    "\n",
    "Y los mejores resultados obtenidos han sido: {'feature_fraction': 0.5272194081485266, 'learning_rate': 0.007956692977969942, 'min_child_samples': 95, 'n_estimators': 950, 'num_leaves': 146}\n",
    "\n",
    "-AUC-ROC del modelo LightGBM optimizado:  0.7697397208645272 \n",
    "-AUC-ROC del modelo LightGBM optimizado con mejores parámetros:  0.7696052052611496\n",
    "\n",
    "-AUC-ROC del modelo LightGBM optimizado con nuevos parámetros:  0.7703544288389735\n",
    "\n",
    "Estos resultados los obtuve pero se perdieron, sin haber hecho cambios aparentes o al menos que yo viese, el resultado fue a peor y no hubo manera de volver a conseguirlo.\n",
    "He añadido más features nuevos en relación a diferentes variables, pero lo más importante, suponiendo una mejora de un 0.005 ha sido la creación de la variable NEIGHBORS_MEAN_TARGET\n",
    "-AUC-ROC del modelo LightGBM optimizado con nuevos parámetros:  0.7769026401223019\n",
    "\n",
    "AUC-ROC del modelo LightGBM optimizado con nuevos parámetros:  0.7731545285625229\n",
    "\n",
    "AUC-ROC del modelo LightGBM optimizado con nuevos parámetros:  0.7711090796313721\n",
    "\n",
    "-Voy a partir desde este punto (1/8) donde he creado algunas variables que me han mejorado el resultado y desde aquí voy a conseguir mejoras\n",
    "\n",
    "AUC-ROC del modelo LightGBM optimizado con nuevos parámetros:  0.772455818453863\n",
    "\n",
    "\n",
    "AUC-ROC del modelo LightGBM optimizado con nuevos parámetros:  0.7729884893816636 (Varios Ratios más añadidos además de pruebas con DAYS_BIRTH Y AGE_INT)\n",
    "\n",
    "AUC-ROC del modelo LightGBM optimizado con nuevos parámetros:  0.7731786026716773 (Algunos ratios comentados y otros he cambiado AGE_INT por DAYS_BIRTH, parece funcionar mejor en algunos casos)\n",
    "\n",
    "AUC-ROC del modelo LightGBM optimizado con nuevos parámetros:  0.7732445545833548 (versión de git más reciente)\n",
    "\n",
    "-Tras realizar una optimización aún más profunda, con más parámetros aún en base a lo investigado y estudiado, con una selección más manual viendo como afecta cada cambio al modelo siguiendo como guía la búsqueda de los mejores hiperparámetros y diversas fuentes, he alcanzado:\n",
    "\n",
    "[1302]\tcv_agg's valid auc: 0.770065 + 0.0048863  Cabe destacar que también he implementado validación cruzada para evitar el sobreajuste al usar siempre los mismo datos. En específico he usado Validación Cruzada K-Fold Estratificada.\n",
    "\n",
    "-Descartes\n",
    "-LightGBM AUC: 0.7656 (Codificación numérica apropiada para algunas variables) (Voy a descartar la idea por el momento porque me ha dado peores resultados todos los modelos)\n",
    "\n",
    "#XGBoost\n",
    "-XGBoost AUC: 0.7491\n",
    "-XGBoost AUC: 0.7481\n",
    "-XGBoost AUC: 0.7481\n",
    "-XGBoost AUC: 0.7488\n",
    "-XGBoost AUC: 0.7491\n",
    "-XGBoost AUC: 0.7599\n",
    "-XGBoost AUC: 0.7588\n",
    "-XGBoost AUC: 0.7578\n",
    "-XGBoost AUC: 0.7589\n",
    "\n",
    "-Mejor AUC-ROC: 0.7634494411242914\n",
    "\n",
    "#Optimización bayesiana tras una primera exploración de los mejores valores: Mejores hiperparámetros:\n",
    "{'colsample_bytree': 0.721, 'gamma': 7.862, 'learning_rate': 0.056, 'max_depth': 6, \n",
    "'min_child_weight': 2, 'n_estimators': 980, 'reg_alpha': 0.054, 'reg_lambda': 4.612, \n",
    "'subsample': 0.616}\n",
    "\n",
    "AUC-ROC del modelo XGBoost optimizado con nuevos parámetros:  0.7683611333318847\n",
    "AUC-ROC del modelo XGBoost optimizado con nuevos parámetros fase 2:  0.7698473737208467\n",
    "\n",
    "-Más selección de variables:\n",
    "AUC-ROC del modelo XGBoost optimizado con nuevos parámetros:  0.7699471304980283\n",
    "AUC-ROC del modelo XGBoost optimizado con nuevos parámetros:  0.770701291804978\n",
    "AUC-ROC del modelo XGBoost optimizado con nuevos parámetros:  0.7703014357865869\n",
    "\n",
    "AUC-ROC del modelo XGBoost optimizado con nuevos parámetros:  0.7718076682618913\n",
    "\n",
    "\n",
    "#Random Forest\n",
    "-Random Forest AUC: 0.7068\n",
    "-Random Forest AUC: 0.7136\n",
    "-Random Forest AUC: 0.7138\n",
    "-Random Forest AUC: 0.7154\n",
    "-Random Forest AUC: 0.7097\n",
    "-Random Forest AUC: 0.7152\n",
    "-Random Forest AUC: 0.7236\n",
    "-Random Forest AUC: 0.7235\n",
    "-Random Forest AUC: 0.7258\n",
    "\n",
    "#Extra Trees\n",
    "-Extra Trees AUC: 0.6981\n",
    "-Extra Trees AUC: 0.7052\n",
    "-Extra Trees AUC: 0.7048\n",
    "-Extra Trees AUC: 0.7038\n",
    "-Extra Trees AUC: 0.7053\n",
    "-Extra Trees AUC: 0.7077\n",
    "-Extra Trees AUC: 0.7262\n",
    "-Extra Trees AUC: 0.7204\n",
    "-Extra Trees AUC: 0.7241\n",
    "\n",
    "#Logistic Regresion\n",
    "-Logistic Regression AUC: 0.7417\n",
    "-Logistic Regression AUC: 0.7478\n",
    "-Logistic Regression AUC: 0.7457\n",
    "-Logistic Regression AUC: 0.7452\n",
    "-Logistic Regression AUC: 0.7459\n",
    "-Logistic Regression AUC: 0.7480\n",
    "-Logistic Regression AUC: 0.7497\n",
    "-Logistic Regression AUC: 0.7500\n",
    "-Logistic Regression AUC: 0.7501\n",
    "\n",
    "#Neural Network\n",
    "-Neural Network AUC: 0.6861\n",
    "-Neural Network AUC: 0.7437 (Como la primera vez tardó demasiado lo descarté, pero tras el último cambio, Variable HAS_CHILDREN y HAS_EMPLOYMENT añadida, he probado a ejecutarla e interrumpirla pronto y me ha dado un buen resultado)\n",
    "\n",
    "#Modelos descartados\n",
    "\n",
    "#KNN (He decidido no seguir haciendo pruebas con KNN. En comparación con los otros modelos los resultados son más bajos y cada vez que refino el proceso empeora)\n",
    "-KNN AUC: 0.5482\n",
    "-KNN AUC: 0.5575\n",
    "-KNN AUC: 0.5497\n",
    "-KNN AUC: 0.5471\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'boosting_type': 'gbdt',  # Fijo\n",
    "    'objective': 'binary',   # Fijo\n",
    "    'metric': 'auc',         # Fijo\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.03),  \n",
    "    'max_bin': hp.quniform('max_bin', 250, 350, 10),\n",
    "    'max_depth': hp.choice('max_depth', [-1, 3, 5, 7]),  \n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 40, 1),  \n",
    "    'min_child_samples': hp.quniform('min_child_samples', 50, 90, 1),  \n",
    "    'subsample': hp.uniform('subsample', 0.9, 1.0),  \n",
    "    'subsample_freq': 1,  # Fijo\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.03, 0.07),  \n",
    "    'min_gain_to_split': hp.uniform('min_gain_to_split', 0.3, 0.7),  \n",
    "    'reg_lambda': hp.uniform('reg_lambda', 90, 110),  \n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 0.1),  \n",
    "    'scale_pos_weight': 1,  # Fijo\n",
    "    'is_unbalance': False  # Fijo\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#Vamos a buscar ahora los mejores hiperparámetros\n",
    "\n",
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params['max_bin'] = int(params['max_bin'])\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['min_child_samples'] = int(params['min_child_samples'])\n",
    "    \n",
    "    # Modelo\n",
    "    model = LGBMClassifier(\n",
    "        boosting_type=params['boosting_type'],\n",
    "        objective=params['objective'],\n",
    "        metric=params['metric'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_bin=params['max_bin'],\n",
    "        max_depth=params['max_depth'],\n",
    "        num_leaves=params['num_leaves'],\n",
    "        min_child_samples=params['min_child_samples'],\n",
    "        subsample=params['subsample'],\n",
    "        subsample_freq=params['subsample_freq'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        min_gain_to_split=params['min_gain_to_split'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        is_unbalance=params['is_unbalance'],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cross-validation para evaluar AUC\n",
    "    auc = cross_val_score(model, X_train, Y_train, scoring='roc_auc', cv=3).mean()\n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<?, ?trial/s, best loss=?]\n",
      "Mejores hiperparámetros:  {'colsample_bytree': 0.06265754420052298, 'learning_rate': 0.027863102676207667, 'max_bin': 320, 'max_depth': 0, 'min_child_samples': 85, 'min_gain_to_split': 0.561172858664214, 'num_leaves': 31, 'reg_alpha': 0.07223073889021282, 'reg_lambda': 98.38320411617768, 'subsample': 0.9996800084660435}\n"
     ]
    }
   ],
   "source": [
    "best = fmin(\n",
    "    fn=objective, #Función objetivo\n",
    "    space=space, #Espacio de búsqueda\n",
    "    algo=tpe.suggest, #Algoritmo TPE\n",
    "    max_evals=50, #Número de evaluaciones\n",
    "    trials=trials, #Almacenamiento de resultados\n",
    "    rstate= np.random.default_rng(42) #Reproducibilidad\n",
    ")\n",
    "\n",
    "#Convertimos los resultados a su formato original\n",
    "\n",
    "best['num_leaves'] = int(best['num_leaves'])\n",
    "best['min_child_samples'] = int(best['min_child_samples'])\n",
    "best['max_depth'] = int(best['max_depth'])\n",
    "best['max_bin'] = int(best['max_bin'])\n",
    "\n",
    "\n",
    "print(\"Mejores hiperparámetros: \", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#Cargar los mejores valores sin tener que recalcularlos\n",
    "with open(\"best_hyperparameters.json\", \"r\") as f:\n",
    "    best = json.load(f)\n",
    "#with open(\"trials_lightgbm_2.pkl\", \"wb\") as f:\n",
    " #   pickle.dump(trials, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 15901, number of negative: 180905\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.146245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 26408\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 257\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 15901, number of negative: 180905\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 26408\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 257\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 15900, number of negative: 180906\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071199 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 26408\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 257\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 15901, number of negative: 180906\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 26408\n",
      "[LightGBM] [Info] Number of data points in the train set: 196807, number of used features: 257\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 15901, number of negative: 180906\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 26408\n",
      "[LightGBM] [Info] Number of data points in the train set: 196807, number of used features: 257\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: colample_bytree\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: min_data_in_group\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=1 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080795 -> initscore=-2.431590\n",
      "[LightGBM] [Info] Start training from score -2.431590\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080795 -> initscore=-2.431590\n",
      "[LightGBM] [Info] Start training from score -2.431590\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080790 -> initscore=-2.431658\n",
      "[LightGBM] [Info] Start training from score -2.431658\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080795 -> initscore=-2.431596\n",
      "[LightGBM] [Info] Start training from score -2.431596\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080795 -> initscore=-2.431596\n",
      "[LightGBM] [Info] Start training from score -2.431596\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1302]\tcv_agg's valid auc: 0.770065 + 0.0048863\n",
      "Key 'auc-mean' not found in eval_hist. Available keys are:  dict_keys(['valid auc-mean', 'valid auc-stdv'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "# Entrenamos el modelo con los mejores hiperparámetros\n",
    "#Muchas de las variables que hay en los datos y que generamos no son muy relevantes para el target. Es muy importante aquellos parámetros\n",
    "#de LightGBM que nos permiten reducir el ruido y regularizar el modelo. Por eso algunas de las más importantes son \n",
    "#freature_fraction y reg_lambda.\n",
    "#model_optimizado = LGBMClassifier(\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',  # Fijo\n",
    "    'objective': 'binary',    # Fijo\n",
    "    'metric': 'auc',          # Fijo\n",
    "    #Usamos los parámetros óptimos de best_hyperparameters.json\n",
    "    #num_leaves < 2^max_depth\n",
    "    'num_leaves': 30,\n",
    "    'learning_rate': 0.02,\n",
    "    #'n_estimators': 950,\n",
    "    'min_child_samples': 70,\n",
    "    'min_child_weight': 0.01,\n",
    "    'min_gain_to_split': 0.007,\n",
    "    'min_data_in_leaf': 400, \n",
    "    'colample_bytree': 0.05,\n",
    "    'max_bin': 300 ,\n",
    "    'feature_fraction': 0.3,\n",
    "    'max_depth': -1,\n",
    "    'bagging_fraction': 1.0,\n",
    "    'bagging_freq': 1,\n",
    "    #'lambda_l1': 22,\n",
    "    #'lambda_l2': 8,\n",
    "    'reg_alpha': 0.0,\n",
    "    'reg_lambda': 100.0,\n",
    "    'subsample':1.0,\n",
    "    'subsample_freq': 1,\n",
    "    'use_label_encoder': False,\n",
    "    'eval_metric': 'auc',\n",
    "    'scale_pos_weight': 1,    # Fijo\n",
    "    'is_unbalance': False,    # Fijo\n",
    "    'random_state': 42        # Fijo\n",
    "}\n",
    "    \n",
    "# Entrenamiento del modelo\n",
    "#model_optimizado.fit(X_train, Y_train)\n",
    "eval_hist = lgb.cv(\n",
    "    params={**lgb_params, 'early_stopping_round': 100, 'verbose_eval': 100, 'cat_smooth': 10, 'max_cat_threshold': 4, 'min_data_in_group': 100},\n",
    "    train_set=lgb.Dataset(X_train, Y_train),\n",
    "    num_boost_round=5000,\n",
    "    nfold=5,\n",
    "    seed=17,\n",
    "    stratified=True\n",
    " )   \n",
    "\n",
    "# Check if 'auc-mean' exists in eval_hist\n",
    "if 'auc-mean' in eval_hist:\n",
    "    print('Highest Average ROC AUC Score (across 5 folds): {}'.format(max(eval_hist['auc-mean'])))\n",
    "    print('Round of Highest Average ROC AUC Score Achieved: {}'.format(np.argmax(eval_hist['auc-mean'])))\n",
    "else:\n",
    "    print(\"Key 'auc-mean' not found in eval_hist. Available keys are: \", eval_hist.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a ver la importancia de las variables en el modelo para sacar ideas de cara al feature engineering\n",
    "lgb.plot_importance(model_optimizado, max_num_features=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimización de hiperparámetros con XGBoost\n",
    "space_xgb = {\n",
    "    'max_depth': hp.quniform('max_depth', 5, 8, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.03), np.log(0.1)),\n",
    "    'n_estimators': hp.quniform('n_estimators', 900, 1200, 10),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 4, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.6, 0.7),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 0.8),\n",
    "    'gamma': hp.loguniform('gamma', np.log(0.1), np.log(10)),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', np.log(0.01), np.log(0.1)),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', np.log(1), np.log(10))\n",
    "}\n",
    "trials_xgb = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb(params):\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "\n",
    "\n",
    "    model_xgb = XGBClassifier(\n",
    "        max_depth=params['max_depth'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=params['n_estimators'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        gamma=params['gamma'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        use_label_encoder=False,\n",
    "        random_state=42,\n",
    "        eval_metric='auc'\n",
    "    )\n",
    "\n",
    "    score = cross_val_score(model_xgb, X_train, Y_train, scoring='roc_auc', cv=3).mean()\n",
    "\n",
    "    return {'loss': -score, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = fmin(\n",
    "    fn=objective_xgb,\n",
    "    space=space_xgb,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=trials_xgb,\n",
    "    rstate= np.random.default_rng(42)\n",
    ")\n",
    "\n",
    "best_xgb['max_depth'] = int(best_xgb['max_depth'])\n",
    "best_xgb['n_estimators'] = int(best_xgb['n_estimators'])\n",
    "best_xgb['min_child_weight'] = int(best_xgb['min_child_weight'])\n",
    "print(\"Mejores hiperparámetros: \", best_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Cargar los mejores valores sin tener que recalcularlos\n",
    "with open(\"best_hyperparameters_xgb.json\", \"r\") as f:\n",
    "    best_xgb = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimizado_xgb = XGBClassifier(\n",
    "    max_depth=best_xgb['max_depth'],\n",
    "    learning_rate=best_xgb['learning_rate'],\n",
    "    n_estimators=best_xgb['n_estimators'],\n",
    "    min_child_weight=best_xgb['min_child_weight'],\n",
    "    subsample=best_xgb['subsample'],\n",
    "    colsample_bytree=best_xgb['colsample_bytree'],\n",
    "    gamma=best_xgb['gamma'],\n",
    "    reg_alpha=best_xgb['reg_alpha'],\n",
    "    reg_lambda=best_xgb['reg_lambda'],\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    eval_metric='auc',\n",
    ")\n",
    "\n",
    "model_optimizado_xgb.fit(X_train, Y_train)\n",
    "auc_roc_xgb = roc_auc_score(Y_test, model_optimizado_xgb.predict_proba(X_test)[:,1])\n",
    "print(\"AUC-ROC del modelo XGBoost optimizado con nuevos parámetros: \", auc_roc_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightGBM\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Remove special characters from column names\n",
    "X_train.columns = X_train.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "X_test.columns = X_test.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "\n",
    "param_dist = {\n",
    "    'num_leaves':[20,31,40,50],\n",
    "    'learning_rate':[0.01,0.05,0.1],\n",
    "    'n_estimators':[100,200,300],\n",
    "    'min_child_samples':[10,20,30],\n",
    "    'feature_fraction':[0.6,0.8,1],\n",
    "    }\n",
    "\n",
    "lgb_model = LGBMClassifier(random_state=42)\n",
    "\n",
    "lgb_random_search = RandomizedSearchCV(estimator=lgb_model, \n",
    "                                       param_distributions=param_dist,\n",
    "                                       n_iter=20,\n",
    "                                       scoring='roc_auc',\n",
    "                                       cv=3,\n",
    "                                       verbose=1,\n",
    "                                       random_state=42)\n",
    "\n",
    "lgb_random_search.fit(X_train, Y_train)\n",
    "\n",
    "#Evaluación\n",
    "print(\"Mejores parámetros:\", lgb_random_search.best_params_)\n",
    "print(\"Mejor AUC-ROC:\", lgb_random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "\n",
    "param_dist = {\n",
    "    'max_depth':[3,5,7,9],\n",
    "    'learning_rate':[0.01,0.05,0.1],\n",
    "    'n_estimators':[100,200,300],\n",
    "    'subsample':[0.6,0.8,1],\n",
    "    'colsample_bytree':[0.6,0.8,1],\n",
    "    'gamma':[0,1,5],\n",
    "    }\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "xgb_random_search = RandomizedSearchCV(estimator=xgb_model, \n",
    "                                       param_distributions=param_dist,\n",
    "                                       n_iter=20,\n",
    "                                       scoring='roc_auc',\n",
    "                                       cv=3,\n",
    "                                       verbose=1,\n",
    "                                       random_state=42)\n",
    "\n",
    "xgb_random_search.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Evaluación\n",
    "print(\"Mejores parámetros:\", xgb_random_search.best_params_)\n",
    "print(\"Mejor AUC-ROC:\", xgb_random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network\n",
    "nn_model = MLPClassifier(random_state=42)\n",
    "nn_model.fit(X_train, Y_train)\n",
    "nn_predictions = nn_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#Evaluación\n",
    "nn_auc = roc_auc_score(Y_test, nn_predictions)\n",
    "print(f\"Neural Network AUC: {nn_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, Y_train)\n",
    "rf_predictions = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#Evaluación\n",
    "rf_auc = roc_auc_score(Y_test, rf_predictions)\n",
    "print(f\"Random Forest AUC: {rf_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra Trees\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "et_model = ExtraTreesClassifier(random_state=42)\n",
    "et_model.fit(X_train, Y_train)\n",
    "et_predictions = et_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#Evaluación\n",
    "et_auc = roc_auc_score(Y_test, et_predictions) \n",
    "print(f\"Extra Trees AUC: {et_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regresión Logística\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train, Y_train)\n",
    "lr_predictions = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#Evaluación\n",
    "lr_auc = roc_auc_score(Y_test, lr_predictions)\n",
    "print(f\"Logistic Regression AUC: {lr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "svm_model = SVC(probability=True, random_state=42)\n",
    "svm_model.fit(X_train, Y_train)\n",
    "svm_predictions = svm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#Evaluación\n",
    "svm_auc = roc_auc_score(Y_test, svm_predictions)\n",
    "print(f\"SVM AUC: {svm_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
